{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating the SparkContext and SparkSession objects to connect to the Spark Cluster\n",
    "\n",
    "You will need to \"connect\" to the cluster in each Jupyter notebook to run the code.\n",
    "\n",
    "The first step is finding the necessary Spark related environment variables using the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "You have to import the `pyspark` module, and to connect to the cluster in this workbook you can run the following cell. \n",
    "\n",
    "You may want to copy the previous cell to other workbooks you start, and you can change the `appName` to a relevant name for your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, HiveContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "spark = SparkSession.builder.appName(\"my-app-name\").getOrCreate() # get the spark session object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous cell creates the essential spark object that you will use to interact with the cluster and direct operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old contexts that are here for backwards compatibility\n",
    "sc           = spark.sparkContext\n",
    "sql_context  = SQLContext(sc)\n",
    "hive_context = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous cell shows you other ways in which you can interact with the cluster. You will not need these for the purposes of this class, but code in version PySpark < 3 will use spark context `sc` instead of the `spark` variable. In PySpark 3+ you can use the spark context and spark session interchangably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Important!: Close the spark session before you close your notebook.**\n",
    "\n",
    "Just as you have to create the spark session object, you need to shut down the session before you exit the Jupyter notebook. If you do not do this, then you will have an abandoned session to the cluster which will have asked for resources and not released them. Remember, Spark uses memory on the cluster, so if the memorys already allocated to another application when you create a spark session object, then Spark will not perform.\n",
    "\n",
    "Closing the session is very easy. (Do not run the following cell unless you want to close the session). \n",
    "\n",
    "This cell has not been added to the example notebooks, it is up to you do do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the previous cell and want you to run the cells below this, then comment out the cell code and restart your kernel to make a new spark session and not close it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of SparkContext and SparkSession\n",
    "\n",
    "** Note: This section was taken verbatim from the book [Fast Data Processing with Spark - 3ed](https://www.packtpub.com/big-data-and-business-intelligence/fast-data-processing-spark-2-third-edition).**\n",
    "\n",
    "Prior to Spark 2.0.0, the three main connection objects were `SparkContext, SqlContext, and HiveContext`. The `SparkContext` object was the connection to a Spark execution environment and created RDDs and others, `SQLContext` worked with `SparkSQL` in the background of `SparkContext`, and `HiveContext` interacted with the Hive stores.\n",
    "\n",
    "Spark 2.0.0 introduced Datasets/DataFrames as the main distributed data abstraction interface and the `SparkSession` object as the entry point to a Spark execution environment. Appropriately, the `SparkSession` object is found in the namespace, `org.apache.spark.sql.SparkSession` (Scala), or `pyspark.sql.sparkSession`. A few points to note are as follows:\n",
    "\n",
    "* In Scala and Java, Datasets form the main data abstraction as typed data; however, for Python and R (which do not have compile time type checking), the data abstraction is DataFrame. For all practical API purposes, the Datasets in Scala/Java are the same as DataFrames in Python/R.\n",
    "* While Datasets/DataFrames are top-level interfaces, RDDs have not disappeared. In fact, the underlying structures are still RRDs. Also, to interact with RDDs, we still need a SparkContext object and we can get one from the SparkSession object.\n",
    "* The `SparkSession` object encapsulates the `SparkContext` object. As of version 2.0.0, `SparkContext` is still the conduit to a Spark cluster (local or remote)\n",
    "\n",
    "However, for operations such as reading and creating Datasets, use the `SparkSession` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see some of the attributes associated with the `SparkSession` and `SparkContext` you can run some methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that your cluster is connected to a cluster using YARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the standard configuration when spinning up spark by running the following commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "conf = SparkConf()\n",
    "print(conf.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can examine the current configuration for your spark application by running the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
