{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf3195c3",
   "metadata": {},
   "source": [
    "# Consuming the AWS Kinesis Stream\n",
    "## Analytics using PySpark Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad446cb",
   "metadata": {},
   "source": [
    "In this notebook we are leveraging a custom Java package to let us connect to Amazon kinesis using PySpark Structured Streaming so we can work with our data using our favorite PySparkSQL commands. We are downloading the package from the Maven Java repository. It is like the repository to download python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db48bf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import time\n",
    "findspark.init()\n",
    "import os\n",
    "\n",
    "# on this line we are installing a custom package that let's us connect to Amazon kinesis\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages=com.qubole.spark/spark-sql-kinesis_2.12/1.2.0_spark-3.0 pyspark-shell'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import col, json_tuple,from_json,get_json_object\n",
    "from pyspark.sql.types import StringType, FloatType\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kinesis import KinesisUtils, InitialPositionInStream\n",
    "from datetime import datetime, timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "166cc91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_name = 'pyspark-kinesis-test' # set this stream name to the same one you used in the producer notebook\n",
    "region_name = 'us-east-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43d4947d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"streaming\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98db7e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-84-70.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0-amzn-0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>streaming</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc5acab8950>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca90fda",
   "metadata": {},
   "source": [
    "Start off by running the following cells to connect to the kinesis stream as a consumer and explore the format of the output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c47717f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "kinesis = spark \\\n",
    "        .readStream \\\n",
    "        .format('kinesis') \\\n",
    "        .option('streamName', stream_name) \\\n",
    "        .option('endpointUrl', f'https://kinesis.{region_name}.amazonaws.com')\\\n",
    "        .option('region', region_name) \\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7f152e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[data: binary, streamName: string, partitionKey: string, sequenceNumber: string, approximateArrivalTimestamp: timestamp]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kinesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9bbdab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- data: binary (nullable = true)\n",
      " |-- streamName: string (nullable = true)\n",
      " |-- partitionKey: string (nullable = true)\n",
      " |-- sequenceNumber: string (nullable = true)\n",
      " |-- approximateArrivalTimestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kinesis.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5e264f",
   "metadata": {},
   "source": [
    "What happens when we run a take method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84726840",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Queries with streaming sources must be executed with writeStream.start();;\nkinesis",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31269/2916868571.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkinesis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m         \"\"\"\n\u001b[0;32m--> 640\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    595\u001b[0m         \"\"\"\n\u001b[1;32m    596\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Queries with streaming sources must be executed with writeStream.start();;\nkinesis"
     ]
    }
   ],
   "source": [
    "kinesis.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f18d50",
   "metadata": {},
   "source": [
    "IT ERRORS! So structured streaming sources are not like normal sources. We need to grab data from the stream and work with it in individual pieces.\n",
    "\n",
    "The following cells will show us how to view some data from the stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bcf7298b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/07 05:30:33 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /mnt/tmp/temporary-f188c4cc-8d57-4ca8-bb27-719428400aa9. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/11/07 05:30:33 WARN StreamingQueryManager: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+------------+--------------------------------------------------------+---------------------------+\n",
      "|data                                                                                                                                                                                                                                                                                                                                                                                                                  |streamName          |partitionKey|sequenceNumber                                          |approximateArrivalTimestamp|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+------------+--------------------------------------------------------+---------------------------+\n",
      "|[7B 22 73 79 6D 62 6F 6C 22 3A 20 22 42 49 4E 41 4E 43 45 3A 42 54 43 55 53 44 54 22 2C 20 22 70 72 69 63 65 5F 6C 61 73 74 22 3A 20 32 30 38 37 38 2E 39 38 2C 20 22 74 72 61 64 65 5F 64 74 22 3A 20 22 31 31 2F 30 37 2F 32 30 32 32 2C 20 30 35 3A 33 30 3A 33 31 2E 31 38 36 30 30 30 22 2C 20 22 76 6F 6C 75 6D 65 22 3A 20 30 2E 30 35 37 32 38 2C 20 22 63 6F 6E 64 69 74 69 6F 6E 73 22 3A 20 6E 75 6C 6C 7D]|pyspark-kinesis-test|part_key    |49634947866736348337619157748105194206898828320664715266|2022-11-07 05:30:33.053    |\n",
      "|[7B 22 73 79 6D 62 6F 6C 22 3A 20 22 42 49 4E 41 4E 43 45 3A 42 54 43 55 53 44 54 22 2C 20 22 70 72 69 63 65 5F 6C 61 73 74 22 3A 20 32 30 38 37 38 2E 39 39 2C 20 22 74 72 61 64 65 5F 64 74 22 3A 20 22 31 31 2F 30 37 2F 32 30 32 32 2C 20 30 35 3A 33 30 3A 33 31 2E 31 39 30 30 30 30 22 2C 20 22 76 6F 6C 75 6D 65 22 3A 20 30 2E 31 32 35 33 2C 20 22 63 6F 6E 64 69 74 69 6F 6E 73 22 3A 20 6E 75 6C 6C 7D]   |pyspark-kinesis-test|part_key    |49634947866736348337619157748106403132718442949839421442|2022-11-07 05:30:33.06     |\n",
      "|[7B 22 73 79 6D 62 6F 6C 22 3A 20 22 42 49 4E 41 4E 43 45 3A 42 54 43 55 53 44 54 22 2C 20 22 70 72 69 63 65 5F 6C 61 73 74 22 3A 20 32 30 38 37 39 2E 33 33 2C 20 22 74 72 61 64 65 5F 64 74 22 3A 20 22 31 31 2F 30 37 2F 32 30 32 32 2C 20 30 35 3A 33 30 3A 33 31 2E 31 39 30 30 30 30 22 2C 20 22 76 6F 6C 75 6D 65 22 3A 20 30 2E 30 30 32 32 2C 20 22 63 6F 6E 64 69 74 69 6F 6E 73 22 3A 20 6E 75 6C 6C 7D]   |pyspark-kinesis-test|part_key    |49634947866736348337619157748107612058538057579014127618|2022-11-07 05:30:33.067    |\n",
      "|[7B 22 73 79 6D 62 6F 6C 22 3A 20 22 42 49 4E 41 4E 43 45 3A 42 54 43 55 53 44 54 22 2C 20 22 70 72 69 63 65 5F 6C 61 73 74 22 3A 20 32 30 38 37 39 2E 33 34 2C 20 22 74 72 61 64 65 5F 64 74 22 3A 20 22 31 31 2F 30 37 2F 32 30 32 32 2C 20 30 35 3A 33 30 3A 33 31 2E 31 39 30 30 30 30 22 2C 20 22 76 6F 6C 75 6D 65 22 3A 20 30 2E 32 38 30 35 34 2C 20 22 63 6F 6E 64 69 74 69 6F 6E 73 22 3A 20 6E 75 6C 6C 7D]|pyspark-kinesis-test|part_key    |49634947866736348337619157748108820984357672208188833794|2022-11-07 05:30:33.074    |\n",
      "|[7B 22 73 79 6D 62 6F 6C 22 3A 20 22 42 49 4E 41 4E 43 45 3A 42 54 43 55 53 44 54 22 2C 20 22 70 72 69 63 65 5F 6C 61 73 74 22 3A 20 32 30 38 37 39 2E 33 35 2C 20 22 74 72 61 64 65 5F 64 74 22 3A 20 22 31 31 2F 30 37 2F 32 30 32 32 2C 20 30 35 3A 33 30 3A 33 31 2E 31 39 30 30 30 30 22 2C 20 22 76 6F 6C 75 6D 65 22 3A 20 30 2E 31 32 2C 20 22 63 6F 6E 64 69 74 69 6F 6E 73 22 3A 20 6E 75 6C 6C 7D]         |pyspark-kinesis-test|part_key    |49634947866736348337619157748110029910177286837363539970|2022-11-07 05:30:33.083    |\n",
      "|[7B 22 73 79 6D 62 6F 6C 22 3A 20 22 42 49 4E 41 4E 43 45 3A 42 54 43 55 53 44 54 22 2C 20 22 70 72 69 63 65 5F 6C 61 73 74 22 3A 20 32 30 38 37 39 2E 35 39 2C 20 22 74 72 61 64 65 5F 64 74 22 3A 20 22 31 31 2F 30 37 2F 32 30 32 32 2C 20 30 35 3A 33 30 3A 33 31 2E 31 39 30 30 30 30 22 2C 20 22 76 6F 6C 75 6D 65 22 3A 20 30 2E 30 30 36 33 31 2C 20 22 63 6F 6E 64 69 74 69 6F 6E 73 22 3A 20 6E 75 6C 6C 7D]|pyspark-kinesis-test|part_key    |49634947866736348337619157748111238835996901466538246146|2022-11-07 05:30:33.09     |\n",
      "|[7B 22 73 79 6D 62 6F 6C 22 3A 20 22 42 49 4E 41 4E 43 45 3A 42 54 43 55 53 44 54 22 2C 20 22 70 72 69 63 65 5F 6C 61 73 74 22 3A 20 32 30 38 37 39 2E 37 35 2C 20 22 74 72 61 64 65 5F 64 74 22 3A 20 22 31 31 2F 30 37 2F 32 30 32 32 2C 20 30 35 3A 33 30 3A 33 31 2E 31 39 30 30 30 30 22 2C 20 22 76 6F 6C 75 6D 65 22 3A 20 30 2E 30 35 31 33 31 2C 20 22 63 6F 6E 64 69 74 69 6F 6E 73 22 3A 20 6E 75 6C 6C 7D]|pyspark-kinesis-test|part_key    |49634947866736348337619157748112447761816516095712952322|2022-11-07 05:30:33.096    |\n",
      "|[7B 22 73 79 6D 62 6F 6C 22 3A 20 22 42 49 4E 41 4E 43 45 3A 42 54 43 55 53 44 54 22 2C 20 22 70 72 69 63 65 5F 6C 61 73 74 22 3A 20 32 30 38 37 39 2E 37 36 2C 20 22 74 72 61 64 65 5F 64 74 22 3A 20 22 31 31 2F 30 37 2F 32 30 32 32 2C 20 30 35 3A 33 30 3A 33 31 2E 31 39 30 30 30 30 22 2C 20 22 76 6F 6C 75 6D 65 22 3A 20 30 2E 33 38 33 2C 20 22 63 6F 6E 64 69 74 69 6F 6E 73 22 3A 20 6E 75 6C 6C 7D]      |pyspark-kinesis-test|part_key    |49634947866736348337619157748113656687636130724887658498|2022-11-07 05:30:33.103    |\n",
      "|[7B 22 73 79 6D 62 6F 6C 22 3A 20 22 42 49 4E 41 4E 43 45 3A 42 54 43 55 53 44 54 22 2C 20 22 70 72 69 63 65 5F 6C 61 73 74 22 3A 20 32 30 38 37 39 2E 37 39 2C 20 22 74 72 61 64 65 5F 64 74 22 3A 20 22 31 31 2F 30 37 2F 32 30 32 32 2C 20 30 35 3A 33 30 3A 33 31 2E 31 39 30 30 30 30 22 2C 20 22 76 6F 6C 75 6D 65 22 3A 20 30 2E 33 36 30 38 32 2C 20 22 63 6F 6E 64 69 74 69 6F 6E 73 22 3A 20 6E 75 6C 6C 7D]|pyspark-kinesis-test|part_key    |49634947866736348337619157748114865613455745354062364674|2022-11-07 05:30:33.11     |\n",
      "|[7B 22 73 79 6D 62 6F 6C 22 3A 20 22 42 49 4E 41 4E 43 45 3A 42 54 43 55 53 44 54 22 2C 20 22 70 72 69 63 65 5F 6C 61 73 74 22 3A 20 32 30 38 37 39 2E 38 2C 20 22 74 72 61 64 65 5F 64 74 22 3A 20 22 31 31 2F 30 37 2F 32 30 32 32 2C 20 30 35 3A 33 30 3A 33 31 2E 31 39 30 30 30 30 22 2C 20 22 76 6F 6C 75 6D 65 22 3A 20 30 2E 30 30 30 35 2C 20 22 63 6F 6E 64 69 74 69 6F 6E 73 22 3A 20 6E 75 6C 6C 7D]      |pyspark-kinesis-test|part_key    |49634947866736348337619157748116074539275359983237070850|2022-11-07 05:30:33.116    |\n",
      "|[7B 22 73 79 6D 62 6F 6C 22 3A 20 22 42 49 4E 41 4E 43 45 3A 42 54 43 55 53 44 54 22 2C 20 22 70 72 69 63 65 5F 6C 61 73 74 22 3A 20 32 30 38 37 39 2E 38 2C 20 22 74 72 61 64 65 5F 64 74 22 3A 20 22 31 31 2F 30 37 2F 32 30 32 32 2C 20 30 35 3A 33 30 3A 33 31 2E 31 39 30 30 30 30 22 2C 20 22 76 6F 6C 75 6D 65 22 3A 20 30 2E 31 32 32 34 37 2C 20 22 63 6F 6E 64 69 74 69 6F 6E 73 22 3A 20 6E 75 6C 6C 7D]   |pyspark-kinesis-test|part_key    |49634947866736348337619157748117283465094974612411777026|2022-11-07 05:30:33.124    |\n",
      "|[7B 22 73 79 6D 62 6F 6C 22 3A 20 22 42 49 4E 41 4E 43 45 3A 42 54 43 55 53 44 54 22 2C 20 22 70 72 69 63 65 5F 6C 61 73 74 22 3A 20 32 30 38 37 39 2E 38 31 2C 20 22 74 72 61 64 65 5F 64 74 22 3A 20 22 31 31 2F 30 37 2F 32 30 32 32 2C 20 30 35 3A 33 30 3A 33 31 2E 31 39 30 30 30 30 22 2C 20 22 76 6F 6C 75 6D 65 22 3A 20 30 2E 30 30 32 33 39 2C 20 22 63 6F 6E 64 69 74 69 6F 6E 73 22 3A 20 6E 75 6C 6C 7D]|pyspark-kinesis-test|part_key    |49634947866736348337619157748118492390914589310305959938|2022-11-07 05:30:33.131    |\n",
      "|[7B 22 73 79 6D 62 6F 6C 22 3A 20 22 42 49 4E 41 4E 43 45 3A 42 54 43 55 53 44 54 22 2C 20 22 70 72 69 63 65 5F 6C 61 73 74 22 3A 20 32 30 38 37 39 2E 38 32 2C 20 22 74 72 61 64 65 5F 64 74 22 3A 20 22 31 31 2F 30 37 2F 32 30 32 32 2C 20 30 35 3A 33 30 3A 33 31 2E 31 39 30 30 30 30 22 2C 20 22 76 6F 6C 75 6D 65 22 3A 20 30 2E 30 30 30 35 31 2C 20 22 63 6F 6E 64 69 74 69 6F 6E 73 22 3A 20 6E 75 6C 6C 7D]|pyspark-kinesis-test|part_key    |49634947866736348337619157748119701316734203939480666114|2022-11-07 05:30:33.142    |\n",
      "|[7B 22 73 79 6D 62 6F 6C 22 3A 20 22 42 49 4E 41 4E 43 45 3A 42 54 43 55 53 44 54 22 2C 20 22 70 72 69 63 65 5F 6C 61 73 74 22 3A 20 32 30 38 37 39 2E 38 32 2C 20 22 74 72 61 64 65 5F 64 74 22 3A 20 22 31 31 2F 30 37 2F 32 30 32 32 2C 20 30 35 3A 33 30 3A 33 31 2E 31 39 30 30 30 30 22 2C 20 22 76 6F 6C 75 6D 65 22 3A 20 30 2E 32 34 39 32 37 2C 20 22 63 6F 6E 64 69 74 69 6F 6E 73 22 3A 20 6E 75 6C 6C 7D]|pyspark-kinesis-test|part_key    |49634947866736348337619157748120910242553818568655372290|2022-11-07 05:30:33.149    |\n",
      "|[7B 22 73 79 6D 62 6F 6C 22 3A 20 22 42 49 4E 41 4E 43 45 3A 42 54 43 55 53 44 54 22 2C 20 22 70 72 69 63 65 5F 6C 61 73 74 22 3A 20 32 30 38 37 39 2E 38 33 2C 20 22 74 72 61 64 65 5F 64 74 22 3A 20 22 31 31 2F 30 37 2F 32 30 32 32 2C 20 30 35 3A 33 30 3A 33 31 2E 31 39 30 30 30 30 22 2C 20 22 76 6F 6C 75 6D 65 22 3A 20 30 2E 30 34 35 31 38 2C 20 22 63 6F 6E 64 69 74 69 6F 6E 73 22 3A 20 6E 75 6C 6C 7D]|pyspark-kinesis-test|part_key    |49634947866736348337619157748122119168373433197830078466|2022-11-07 05:30:33.156    |\n",
      "|[7B 22 73 79 6D 62 6F 6C 22 3A 20 22 42 49 4E 41 4E 43 45 3A 42 54 43 55 53 44 54 22 2C 20 22 70 72 69 63 65 5F 6C 61 73 74 22 3A 20 32 30 38 37 39 2E 38 33 2C 20 22 74 72 61 64 65 5F 64 74 22 3A 20 22 31 31 2F 30 37 2F 32 30 32 32 2C 20 30 35 3A 33 30 3A 33 31 2E 31 39 30 30 30 30 22 2C 20 22 76 6F 6C 75 6D 65 22 3A 20 30 2E 30 34 32 31 33 2C 20 22 63 6F 6E 64 69 74 69 6F 6E 73 22 3A 20 6E 75 6C 6C 7D]|pyspark-kinesis-test|part_key    |49634947866736348337619157748123328094193047827004784642|2022-11-07 05:30:33.162    |\n",
      "|[7B 22 73 79 6D 62 6F 6C 22 3A 20 22 42 49 4E 41 4E 43 45 3A 42 54 43 55 53 44 54 22 2C 20 22 70 72 69 63 65 5F 6C 61 73 74 22 3A 20 32 30 38 37 38 2E 39 36 2C 20 22 74 72 61 64 65 5F 64 74 22 3A 20 22 31 31 2F 30 37 2F 32 30 32 32 2C 20 30 35 3A 33 30 3A 33 31 2E 32 30 30 30 30 30 22 2C 20 22 76 6F 6C 75 6D 65 22 3A 20 30 2E 30 33 39 32 38 2C 20 22 63 6F 6E 64 69 74 69 6F 6E 73 22 3A 20 6E 75 6C 6C 7D]|pyspark-kinesis-test|part_key    |49634947866736348337619157748124537020012662456179490818|2022-11-07 05:30:33.168    |\n",
      "|[7B 22 73 79 6D 62 6F 6C 22 3A 20 22 42 49 4E 41 4E 43 45 3A 42 54 43 55 53 44 54 22 2C 20 22 70 72 69 63 65 5F 6C 61 73 74 22 3A 20 32 30 38 37 38 2E 39 36 2C 20 22 74 72 61 64 65 5F 64 74 22 3A 20 22 31 31 2F 30 37 2F 32 30 32 32 2C 20 30 35 3A 33 30 3A 33 31 2E 32 30 30 30 30 30 22 2C 20 22 76 6F 6C 75 6D 65 22 3A 20 30 2E 30 37 30 35 31 2C 20 22 63 6F 6E 64 69 74 69 6F 6E 73 22 3A 20 6E 75 6C 6C 7D]|pyspark-kinesis-test|part_key    |49634947866736348337619157748125745945832277085354196994|2022-11-07 05:30:33.176    |\n",
      "|[7B 22 73 79 6D 62 6F 6C 22 3A 20 22 42 49 4E 41 4E 43 45 3A 42 54 43 55 53 44 54 22 2C 20 22 70 72 69 63 65 5F 6C 61 73 74 22 3A 20 32 30 38 37 38 2E 39 36 2C 20 22 74 72 61 64 65 5F 64 74 22 3A 20 22 31 31 2F 30 37 2F 32 30 32 32 2C 20 30 35 3A 33 30 3A 33 31 2E 32 30 30 30 30 30 22 2C 20 22 76 6F 6C 75 6D 65 22 3A 20 30 2E 30 31 33 35 39 2C 20 22 63 6F 6E 64 69 74 69 6F 6E 73 22 3A 20 6E 75 6C 6C 7D]|pyspark-kinesis-test|part_key    |49634947866736348337619157748126954871651891714528903170|2022-11-07 05:30:33.183    |\n",
      "|[7B 22 73 79 6D 62 6F 6C 22 3A 20 22 42 49 4E 41 4E 43 45 3A 42 54 43 55 53 44 54 22 2C 20 22 70 72 69 63 65 5F 6C 61 73 74 22 3A 20 32 30 38 37 39 2E 32 35 2C 20 22 74 72 61 64 65 5F 64 74 22 3A 20 22 31 31 2F 30 37 2F 32 30 32 32 2C 20 30 35 3A 33 30 3A 33 31 2E 32 30 30 30 30 30 22 2C 20 22 76 6F 6C 75 6D 65 22 3A 20 30 2E 31 31 37 36 34 2C 20 22 63 6F 6E 64 69 74 69 6F 6E 73 22 3A 20 6E 75 6C 6C 7D]|pyspark-kinesis-test|part_key    |49634947866736348337619157748128163797471506343703609346|2022-11-07 05:30:33.19     |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+------------+--------------------------------------------------------+---------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kinesis_simple = kinesis\n",
    "(\n",
    "    kinesis_simple.writeStream\n",
    "    .format(\"console\")\n",
    "    .option(\"truncate\", \"false\")\n",
    "    .trigger(once=True)\n",
    "    .start()\n",
    "    .awaitTermination()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ddcd32",
   "metadata": {},
   "source": [
    "The data is mostly in binary format! That won't be helpful for our analysis. We need to cast the data column into a string. The following cell does this by running a PySparkSQL command on the streaming dataframe to take only the data column and convert it to a string. Everyone should be familiar with this command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a92cc135",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/07 05:30:47 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /mnt/tmp/temporary-7a36d165-bf6b-4695-b96f-e781ed217cb3. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/11/07 05:30:47 WARN StreamingQueryManager: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|data                                                                                                                                   |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{\"symbol\": \"BINANCE:BTCUSDT\", \"price_last\": 20882.25, \"trade_dt\": \"11/07/2022, 05:30:45.818000\", \"volume\": 0.00021, \"conditions\": null}|\n",
      "|{\"symbol\": \"BINANCE:BTCUSDT\", \"price_last\": 20882.24, \"trade_dt\": \"11/07/2022, 05:30:45.818000\", \"volume\": 0.00111, \"conditions\": null}|\n",
      "|{\"symbol\": \"BINANCE:BTCUSDT\", \"price_last\": 20881.86, \"trade_dt\": \"11/07/2022, 05:30:45.818000\", \"volume\": 0.00266, \"conditions\": null}|\n",
      "|{\"symbol\": \"BINANCE:BTCUSDT\", \"price_last\": 20881.87, \"trade_dt\": \"11/07/2022, 05:30:45.899000\", \"volume\": 0.09578, \"conditions\": null}|\n",
      "|{\"symbol\": \"BINANCE:BTCUSDT\", \"price_last\": 20881.86, \"trade_dt\": \"11/07/2022, 05:30:45.899000\", \"volume\": 0.02129, \"conditions\": null}|\n",
      "|{\"symbol\": \"BINANCE:BTCUSDT\", \"price_last\": 20882.26, \"trade_dt\": \"11/07/2022, 05:30:45.960000\", \"volume\": 0.00466, \"conditions\": null}|\n",
      "|{\"symbol\": \"BINANCE:BTCUSDT\", \"price_last\": 20881.88, \"trade_dt\": \"11/07/2022, 05:30:45.962000\", \"volume\": 0.00314, \"conditions\": null}|\n",
      "|{\"symbol\": \"BINANCE:BTCUSDT\", \"price_last\": 20881.88, \"trade_dt\": \"11/07/2022, 05:30:45.965000\", \"volume\": 0.00928, \"conditions\": null}|\n",
      "|{\"symbol\": \"BINANCE:BTCUSDT\", \"price_last\": 20881.9, \"trade_dt\": \"11/07/2022, 05:30:46.078000\", \"volume\": 0.04711, \"conditions\": null} |\n",
      "|{\"symbol\": \"BINANCE:BTCUSDT\", \"price_last\": 20881.9, \"trade_dt\": \"11/07/2022, 05:30:46.137000\", \"volume\": 0.0083, \"conditions\": null}  |\n",
      "|{\"symbol\": \"BINANCE:BTCUSDT\", \"price_last\": 20882.2, \"trade_dt\": \"11/07/2022, 05:30:46.254000\", \"volume\": 0.00463, \"conditions\": null} |\n",
      "|{\"symbol\": \"BINANCE:BTCUSDT\", \"price_last\": 20881.92, \"trade_dt\": \"11/07/2022, 05:30:46.269000\", \"volume\": 0.01369, \"conditions\": null}|\n",
      "|{\"symbol\": \"BINANCE:BTCUSDT\", \"price_last\": 20881.92, \"trade_dt\": \"11/07/2022, 05:30:46.329000\", \"volume\": 0.00059, \"conditions\": null}|\n",
      "|{\"symbol\": \"BINANCE:BTCUSDT\", \"price_last\": 20881.92, \"trade_dt\": \"11/07/2022, 05:30:46.333000\", \"volume\": 0.00059, \"conditions\": null}|\n",
      "|{\"symbol\": \"BINANCE:BTCUSDT\", \"price_last\": 20881.92, \"trade_dt\": \"11/07/2022, 05:30:46.350000\", \"volume\": 0.00059, \"conditions\": null}|\n",
      "|{\"symbol\": \"BINANCE:BTCUSDT\", \"price_last\": 20882.2, \"trade_dt\": \"11/07/2022, 05:30:46.365000\", \"volume\": 0.00421, \"conditions\": null} |\n",
      "|{\"symbol\": \"BINANCE:BTCUSDT\", \"price_last\": 20882.2, \"trade_dt\": \"11/07/2022, 05:30:46.385000\", \"volume\": 0.00419, \"conditions\": null} |\n",
      "|{\"symbol\": \"BINANCE:BTCUSDT\", \"price_last\": 20881.92, \"trade_dt\": \"11/07/2022, 05:30:46.411000\", \"volume\": 0.00498, \"conditions\": null}|\n",
      "|{\"symbol\": \"BINANCE:BTCUSDT\", \"price_last\": 20881.88, \"trade_dt\": \"11/07/2022, 05:30:46.424000\", \"volume\": 0.00239, \"conditions\": null}|\n",
      "|{\"symbol\": \"BINANCE:BTCUSDT\", \"price_last\": 20881.87, \"trade_dt\": \"11/07/2022, 05:30:46.440000\", \"volume\": 0.00835, \"conditions\": null}|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "kinesis.select(col('data').cast('string'))\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .option(\"truncate\", \"false\")\n",
    "    .trigger(once=True)\n",
    "    .start()\n",
    "    .awaitTermination()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6600bc",
   "metadata": {},
   "source": [
    "We want to save this change, so the following cell will make a new dataframe object with just the string `data` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca93a91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kinesis_datastring = kinesis.select(col('data').cast('string'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3eaab2",
   "metadata": {},
   "source": [
    "This cell shows us once more the format of the `data` column. It is a string representation of a JSON object, which is very similar to a dictionary in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9392d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/07 05:30:53 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /mnt/tmp/temporary-e7f09c97-6362-46bf-9818-407042665def. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/11/07 05:30:53 WARN StreamingQueryManager: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|data                                                                                                                                   |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{\"symbol\": \"BINANCE:BTCUSDT\", \"price_last\": 20886.59, \"trade_dt\": \"11/07/2022, 05:30:50.694000\", \"volume\": 0.001, \"conditions\": null}  |\n",
      "|{\"symbol\": \"BINANCE:BTCUSDT\", \"price_last\": 20886.59, \"trade_dt\": \"11/07/2022, 05:30:50.697000\", \"volume\": 0.019, \"conditions\": null}  |\n",
      "|{\"symbol\": \"BINANCE:BTCUSDT\", \"price_last\": 20886.59, \"trade_dt\": \"11/07/2022, 05:30:50.706000\", \"volume\": 0.01, \"conditions\": null}   |\n",
      "|{\"symbol\": \"BINANCE:BTCUSDT\", \"price_last\": 20886.59, \"trade_dt\": \"11/07/2022, 05:30:50.706000\", \"volume\": 0.019, \"conditions\": null}  |\n",
      "|{\"symbol\": \"BINANCE:BTCUSDT\", \"price_last\": 20886.58, \"trade_dt\": \"11/07/2022, 05:30:50.708000\", \"volume\": 0.04749, \"conditions\": null}|\n",
      "|{\"symbol\": \"BINANCE:BTCUSDT\", \"price_last\": 20886.7, \"trade_dt\": \"11/07/2022, 05:30:50.716000\", \"volume\": 0.22887, \"conditions\": null} |\n",
      "|{\"symbol\": \"BINANCE:BTCUSDT\", \"price_last\": 20886.69, \"trade_dt\": \"11/07/2022, 05:30:50.716000\", \"volume\": 0.14158, \"conditions\": null}|\n",
      "|{\"symbol\": \"BINANCE:BTCUSDT\", \"price_last\": 20886.59, \"trade_dt\": \"11/07/2022, 05:30:50.716000\", \"volume\": 0.0584, \"conditions\": null} |\n",
      "|{\"symbol\": \"BINANCE:BTCUSDT\", \"price_last\": 20886.59, \"trade_dt\": \"11/07/2022, 05:30:50.718000\", \"volume\": 0.08318, \"conditions\": null}|\n",
      "|{\"symbol\": \"BINANCE:BTCUSDT\", \"price_last\": 20886.39, \"trade_dt\": \"11/07/2022, 05:30:50.718000\", \"volume\": 0.14158, \"conditions\": null}|\n",
      "|{\"symbol\": \"BINANCE:BTCUSDT\", \"price_last\": 20886.33, \"trade_dt\": \"11/07/2022, 05:30:50.718000\", \"volume\": 0.00909, \"conditions\": null}|\n",
      "|{\"symbol\": \"BINANCE:BTCUSDT\", \"price_last\": 20886.33, \"trade_dt\": \"11/07/2022, 05:30:50.718000\", \"volume\": 0.14158, \"conditions\": null}|\n",
      "|{\"symbol\": \"BINANCE:BTCUSDT\", \"price_last\": 20886.33, \"trade_dt\": \"11/07/2022, 05:30:50.718000\", \"volume\": 0.00702, \"conditions\": null}|\n",
      "|{\"symbol\": \"BINANCE:BTCUSDT\", \"price_last\": 20886.32, \"trade_dt\": \"11/07/2022, 05:30:50.718000\", \"volume\": 0.14158, \"conditions\": null}|\n",
      "|{\"symbol\": \"BINANCE:BTCUSDT\", \"price_last\": 20886.32, \"trade_dt\": \"11/07/2022, 05:30:50.718000\", \"volume\": 0.00956, \"conditions\": null}|\n",
      "|{\"symbol\": \"BINANCE:BTCUSDT\", \"price_last\": 20886.71, \"trade_dt\": \"11/07/2022, 05:30:50.720000\", \"volume\": 0.00057, \"conditions\": null}|\n",
      "|{\"symbol\": \"BINANCE:BTCUSDT\", \"price_last\": 20886.73, \"trade_dt\": \"11/07/2022, 05:30:50.720000\", \"volume\": 0.00057, \"conditions\": null}|\n",
      "|{\"symbol\": \"BINANCE:BTCUSDT\", \"price_last\": 20886.8, \"trade_dt\": \"11/07/2022, 05:30:50.720000\", \"volume\": 0.0018, \"conditions\": null}  |\n",
      "|{\"symbol\": \"BINANCE:BTCUSDT\", \"price_last\": 20886.8, \"trade_dt\": \"11/07/2022, 05:30:50.720000\", \"volume\": 0.0006, \"conditions\": null}  |\n",
      "|{\"symbol\": \"BINANCE:BTCUSDT\", \"price_last\": 20886.93, \"trade_dt\": \"11/07/2022, 05:30:50.720000\", \"volume\": 0.001, \"conditions\": null}  |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "kinesis_datastring\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .option(\"truncate\", \"false\")\n",
    "    .trigger(once=True)\n",
    "    .start()\n",
    "    .awaitTermination()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0c9543",
   "metadata": {},
   "source": [
    "The following cell will extract the values from the JSON object in a structured manner instead of using regex. Note the functions being used here. Also note the three different methods we can reference the `data` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1354c012",
   "metadata": {},
   "outputs": [],
   "source": [
    "kinesis_processed = kinesis_datastring.select(\n",
    "                                                get_json_object(kinesis_datastring.data,'$.symbol').alias('symbol'),\n",
    "                                                get_json_object(col('data'),'$.price_last').alias('price_last'),\n",
    "                                                get_json_object(kinesis_datastring['data'],'$.trade_dt').alias('trade_dt'),\n",
    "                                                get_json_object(col('data'),'$.volume').alias('volume'),\n",
    "                                                get_json_object(col('data'),'$.conditions').alias('conditions')\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f462ca6",
   "metadata": {},
   "source": [
    "Just like we review the output of our code using the `take()` method for a regular dataframe, the following code is needed to check the output of our structured streaming code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "68e1cb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/07 05:30:59 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /mnt/tmp/temporary-a7f82eed-fc7d-4664-b339-743a954b11cc. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/11/07 05:30:59 WARN StreamingQueryManager: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---------------+----------+---------------------------+-------+----------+\n",
      "|symbol         |price_last|trade_dt                   |volume |conditions|\n",
      "+---------------+----------+---------------------------+-------+----------+\n",
      "|BINANCE:BTCUSDT|20884.51  |11/07/2022, 05:30:58.177000|0.00478|null      |\n",
      "|BINANCE:BTCUSDT|20884.51  |11/07/2022, 05:30:58.228000|0.01972|null      |\n",
      "|BINANCE:BTCUSDT|20884.48  |11/07/2022, 05:30:58.256000|0.02093|null      |\n",
      "|BINANCE:BTCUSDT|20884.5   |11/07/2022, 05:30:58.399000|0.00946|null      |\n",
      "|BINANCE:BTCUSDT|20884.5   |11/07/2022, 05:30:58.439000|1.0E-5 |null      |\n",
      "|BINANCE:BTCUSDT|20884.5   |11/07/2022, 05:30:58.479000|0.009  |null      |\n",
      "|BINANCE:BTCUSDT|20884.65  |11/07/2022, 05:30:58.486000|9.0E-4 |null      |\n",
      "|BINANCE:BTCUSDT|20884.24  |11/07/2022, 05:30:58.500000|6.4E-4 |null      |\n",
      "|BINANCE:BTCUSDT|20884.5   |11/07/2022, 05:30:58.666000|0.00412|null      |\n",
      "|BINANCE:BTCUSDT|20884.49  |11/07/2022, 05:30:58.709000|0.00768|null      |\n",
      "|BINANCE:BTCUSDT|20884.5   |11/07/2022, 05:30:58.744000|0.00407|null      |\n",
      "|BINANCE:BTCUSDT|20884.46  |11/07/2022, 05:30:58.829000|0.00577|null      |\n",
      "|BINANCE:BTCUSDT|20884.46  |11/07/2022, 05:30:58.829000|4.4E-4 |null      |\n",
      "|BINANCE:BTCUSDT|20884.5   |11/07/2022, 05:30:58.831000|1.0E-5 |null      |\n",
      "|BINANCE:BTCUSDT|20884.5   |11/07/2022, 05:30:58.831000|0.00811|null      |\n",
      "|BINANCE:BTCUSDT|20884.5   |11/07/2022, 05:30:58.842000|1.0E-5 |null      |\n",
      "|BINANCE:BTCUSDT|20884.65  |11/07/2022, 05:30:58.842000|0.00177|null      |\n",
      "|BINANCE:BTCUSDT|20884.47  |11/07/2022, 05:30:58.917000|0.00239|null      |\n",
      "|BINANCE:BTCUSDT|20884.47  |11/07/2022, 05:30:58.917000|0.00239|null      |\n",
      "|BINANCE:BTCUSDT|20884.24  |11/07/2022, 05:30:58.917000|0.00242|null      |\n",
      "+---------------+----------+---------------------------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(kinesis_processed\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .option(\"truncate\", \"false\")\n",
    "    .trigger(once=True)\n",
    "    .start()\n",
    "    .awaitTermination()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d55bf98",
   "metadata": {},
   "source": [
    "#### **TO-DO: Convert the data types for the streaming data such that price_last and volume are float type and trade_dt is a timestamp type**\n",
    "\n",
    "The cell provided has most of the code from the last transformation, but we are missing the data type conversion.\n",
    "\n",
    "What is the method to `cast` variables to different data types? And yes, these same command will work with Structured Streaming too!\n",
    "\n",
    "Modify the code below to accomplish this goal. Note that you will have to customize the datetime formatting. See [this link](https://sparkbyexamples.com/spark/pyspark-to_timestamp-convert-string-to-timestamp-type/) for examples and more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c253600-1105-4b5e-bc77-7b87a0c2b906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "698d1059",
   "metadata": {},
   "outputs": [],
   "source": [
    "kinesis_processed = kinesis_datastring.select(\n",
    "                                                get_json_object(kinesis_datastring.data,'$.symbol').alias('symbol'),\n",
    "                                                get_json_object(kinesis_datastring.data,'$.price_last').alias('price_last'),\n",
    "                                                get_json_object(kinesis_datastring.data,'$.trade_dt').alias('trade_dt'),\n",
    "                                                get_json_object(kinesis_datastring.data,'$.volume').alias('volume'),\n",
    "                                                get_json_object(kinesis_datastring.data,'$.conditions').alias('conditions')\n",
    "                                             )\n",
    "\n",
    "kinesis_processed = kinesis_processed.withColumn('trade_dt', to_timestamp('trade_dt', 'MM/dd/yyyy, HH:mm:ss.SSSSSS'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b8534104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- symbol: string (nullable = true)\n",
      " |-- price_last: string (nullable = true)\n",
      " |-- trade_dt: timestamp (nullable = true)\n",
      " |-- volume: string (nullable = true)\n",
      " |-- conditions: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kinesis_processed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c271ef1",
   "metadata": {},
   "source": [
    "Confirm that your schema change worked by printing a single batch of the stream using the cell below. **Remember, sometimes you have to re-run the cell if you get zero rows back**\n",
    "\n",
    "If the code did not work, then you will get a **null** in the `trade_dt` column. You have to get the string format for the string to timestamp conversion EXACTLY right!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8fc05a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/07 05:31:07 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /mnt/tmp/temporary-ef2626ae-d782-4245-af7d-5c9d759dd636. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/11/07 05:31:07 WARN StreamingQueryManager: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---------------+----------+-----------------------+-------+----------+\n",
      "|symbol         |price_last|trade_dt               |volume |conditions|\n",
      "+---------------+----------+-----------------------+-------+----------+\n",
      "|BINANCE:BTCUSDT|20884.89  |2022-11-07 05:31:04.061|0.04   |null      |\n",
      "|BINANCE:BTCUSDT|20884.88  |2022-11-07 05:31:04.061|9.9E-4 |null      |\n",
      "|BINANCE:BTCUSDT|20884.5   |2022-11-07 05:31:04.061|0.00134|null      |\n",
      "|BINANCE:BTCUSDT|20884.45  |2022-11-07 05:31:04.061|0.36082|null      |\n",
      "|BINANCE:BTCUSDT|20884.44  |2022-11-07 05:31:04.061|0.06158|null      |\n",
      "|BINANCE:BTCUSDT|20884.43  |2022-11-07 05:31:04.061|0.01281|null      |\n",
      "|BINANCE:BTCUSDT|20884.43  |2022-11-07 05:31:04.067|0.00897|null      |\n",
      "|BINANCE:BTCUSDT|20884.4   |2022-11-07 05:31:04.067|0.35185|null      |\n",
      "|BINANCE:BTCUSDT|20884.55  |2022-11-07 05:31:04.092|0.00693|null      |\n",
      "|BINANCE:BTCUSDT|20884.44  |2022-11-07 05:31:04.092|0.06158|null      |\n",
      "|BINANCE:BTCUSDT|20884.33  |2022-11-07 05:31:04.101|0.00529|null      |\n",
      "|BINANCE:BTCUSDT|20884.28  |2022-11-07 05:31:04.101|0.37118|null      |\n",
      "|BINANCE:BTCUSDT|20884.27  |2022-11-07 05:31:04.101|0.44076|null      |\n",
      "|BINANCE:BTCUSDT|20884.26  |2022-11-07 05:31:04.101|0.00635|null      |\n",
      "|BINANCE:BTCUSDT|20884.22  |2022-11-07 05:31:04.101|0.02394|null      |\n",
      "|BINANCE:BTCUSDT|20884.17  |2022-11-07 05:31:04.101|0.00239|null      |\n",
      "|BINANCE:BTCUSDT|20884.17  |2022-11-07 05:31:04.101|0.00239|null      |\n",
      "|BINANCE:BTCUSDT|20884.13  |2022-11-07 05:31:04.101|0.02644|null      |\n",
      "|BINANCE:BTCUSDT|20885.29  |2022-11-07 05:31:04.133|0.00177|null      |\n",
      "|BINANCE:BTCUSDT|20884.47  |2022-11-07 05:31:04.143|0.00479|null      |\n",
      "+---------------+----------+-----------------------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(kinesis_processed\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .option('truncate',False)\n",
    "    .trigger(once=True)\n",
    "    .start()\n",
    "    .awaitTermination()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a8dd58",
   "metadata": {},
   "source": [
    "### Sending streaming data to in-memory dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dcfcb0",
   "metadata": {},
   "source": [
    "We can send the output of the kinesis stream into a sql table that we can query and turn into a PySparkSQL dataframe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c925250b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/07 05:31:22 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /mnt/tmp/temporary-f2c2c8f4-47ac-41cd-bb82-00a172c92b12. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/11/07 05:31:22 WARN StreamingQueryManager: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7fc580695590>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "kinesis_processed\n",
    "    .writeStream\n",
    "    .queryName(\"data_stream\") # the name of the table\n",
    "    .format(\"memory\") # the data is being sent as memory\n",
    "    .trigger(once=True)\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ead9f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stream = spark.sql(\"select * from data_stream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f52b07",
   "metadata": {},
   "source": [
    "Remember, the following take command may return no data at all. If so then re-run the **writestream cell** to get a batch with data.\n",
    "\n",
    "Once you run the command you should see the appropriately typed columns you changed in a prior cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b6aa8147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(symbol='BINANCE:BTCUSDT', price_last='20884.61', trade_dt=datetime.datetime(2022, 11, 7, 5, 31, 21, 669000), volume='0.00539', conditions=None),\n",
       " Row(symbol='BINANCE:BTCUSDT', price_last='20884.61', trade_dt=datetime.datetime(2022, 11, 7, 5, 31, 21, 669000), volume='0.00518', conditions=None),\n",
       " Row(symbol='BINANCE:BTCUSDT', price_last='20884.62', trade_dt=datetime.datetime(2022, 11, 7, 5, 31, 21, 669000), volume='0.00478', conditions=None),\n",
       " Row(symbol='BINANCE:BTCUSDT', price_last='20884.63', trade_dt=datetime.datetime(2022, 11, 7, 5, 31, 21, 669000), volume='0.00519', conditions=None),\n",
       " Row(symbol='BINANCE:BTCUSDT', price_last='20884.63', trade_dt=datetime.datetime(2022, 11, 7, 5, 31, 21, 669000), volume='0.00497', conditions=None)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stream.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a4d87c",
   "metadata": {},
   "source": [
    "### **Save a five row dataset as `dict_stream_5` for the final solution json.** **Make sure that it has data in all five rows!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "df83360e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_stream_5 = df_stream.limit(5).toPandas().to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e3aa7d",
   "metadata": {},
   "source": [
    "### Make the stream append to a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5147951",
   "metadata": {},
   "source": [
    "In the cell below we are executing the same streaming command without limiting our trigger to execute once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5912841b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/07 05:31:37 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /mnt/tmp/temporary-f9856ed6-e3cf-4f43-bdc2-28d4a2248047. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/11/07 05:31:37 WARN StreamingQueryManager: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7fc580592fd0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "kinesis_processed\n",
    "    .writeStream\n",
    "    .queryName(\"data_stream\")\n",
    "    .format(\"memory\")\n",
    "    #.trigger(once=True)\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a60be5",
   "metadata": {},
   "source": [
    "Note that we can choose various trigger options based on the nature of the streaming data coming back to us. Check out this site that describes trigger choices for the trigger argument: https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/Trigger.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9c3fb989",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stream = spark.sql(\"select * from data_stream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9ca3a180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-07 05:31:43.542380 - size of dataframe: 705\n",
      "2022-11-07 05:31:47.251601 - size of dataframe: 1150\n",
      "2022-11-07 05:31:50.575749 - size of dataframe: 1613\n",
      "2022-11-07 05:31:53.791109 - size of dataframe: 1995\n",
      "2022-11-07 05:31:56.987120 - size of dataframe: 2481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-07 05:32:00.214704 - size of dataframe: 2925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-07 05:32:03.451122 - size of dataframe: 3348\n",
      "2022-11-07 05:32:06.695119 - size of dataframe: 3716\n",
      "2022-11-07 05:32:09.955126 - size of dataframe: 4086\n",
      "2022-11-07 05:32:13.199494 - size of dataframe: 4556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f'{datetime.now()} - size of dataframe: {df_stream.count()}')\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f718ae82",
   "metadata": {},
   "source": [
    "### Analytics on growing dataframe **(TO-DO)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ae3caf",
   "metadata": {},
   "source": [
    "1. Using the `timedelta` function from the datetime library, find the average price within the last 5 seconds.\n",
    "        \n",
    "    - `filter` to only rows with a datetime within the last 5 seconds. Check out [this link](https://www.geeksforgeeks.org/python-datetime-timedelta-function/) for a hint at a function you could use\n",
    "    - Use the `agg` function to take summary stats of the data in the last 5 seconds\n",
    "        - Include the following stats: average price, number of trades, and latest timestamp of trade\n",
    "        \n",
    "The resulting table should look like:\n",
    "\n",
    "\n",
    "|mean_price_sec5|num_trades|last_trade_dt          |\n",
    "|---------------|----------|-----------------------|\n",
    "|41744.988      |5         |2022-01-09 06:07:41.492|\n",
    "\n",
    "\n",
    "2. Save the resulting table into a Pandas DataFrame then a dictionary.\n",
    "3. Next, use a `for loop` so your average price summary executes once every 5 seconds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4c4b49c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "ini_time_for_now = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "23e3bcad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+-----------------------+\n",
      "|mean_price_sec5   |num_trades|last_trade_dt          |\n",
      "+------------------+----------+-----------------------+\n",
      "|20891.968617886167|1599      |2022-11-07 05:35:19.523|\n",
      "+------------------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(df_stream.filter(ini_time_for_now - timedelta(seconds = 5) <= col(\"trade_dt\"))\\\n",
    "        .agg(avg(col(\"price_last\")).alias('mean_price_sec5'),\\\n",
    "            count(col(\"price_last\")).alias('num_trades'),\\\n",
    "            max(col(\"trade_dt\")).alias('last_trade_dt'))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e9c00691-b705-4eff-bfc2-f2e1dc9a40f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sum = (df_stream.filter(ini_time_for_now - timedelta(seconds = 5) <= col(\"trade_dt\"))\\\n",
    "                    .agg(avg(col(\"price_last\")).alias('mean_price_sec5'),\\\n",
    "                        count(col(\"price_last\")).alias('num_trades'),\\\n",
    "                        max(col(\"trade_dt\")).alias('last_trade_dt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fe786a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dict_stream_summary = data_sum.toPandas().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1fc9cb2c-e5c1-4e09-ba1a-00a2269a25aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-07 05:42:36.403679 - output 1 summary: 1\n",
      "2022-11-07 05:42:39.992288 - output 1 summary: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-07 05:42:43.611107 - output 1 summary: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-07 05:42:47.678548 - output 1 summary: 1\n",
      "2022-11-07 05:42:51.599116 - output 1 summary: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    ini_time_for_now = datetime.now()\n",
    "    print(f'{ini_time_for_now} - output 1 summary: {data_sum.count()}')\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2669ace3",
   "metadata": {},
   "source": [
    "## **Save your analytics results to a json object - then add, commit, and push your notebook and json to GitHub!**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7ab22dc3-d7ad-4446-ab7b-db117578106a",
   "metadata": {},
   "source": [
    "import json\n",
    "json.dump({'dict_stream_5' : str(dict_stream_5),\n",
    "           'dict_stream_summary' : str(dict_stream_summary)\n",
    "          }\n",
    "          , fp = open('soln.json','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ed2afc",
   "metadata": {},
   "source": [
    "# MAKE SURE TO STOP YOUR CLUSTER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d9dd46-6fd6-4ff6-8ea3-bc496cde0ce3",
   "metadata": {},
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4c8cae",
   "metadata": {},
   "source": [
    "# MAKE SURE YOU DELETE YOUR KINESIS STREAM IN THE PRODUCER NOTEBOOK"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
