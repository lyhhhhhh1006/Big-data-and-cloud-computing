{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iqhrT47FOMLd"
   },
   "source": [
    "# Lab: Spark for Text Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First portion adapted from Jonsnowlabs [example notebook](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/SENTIMENT_EN.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "koJZnWQNNPD_"
   },
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The package we need (sparknlp) is not a standard package we have in the bootstrap script. We need to install it! This package is only required for the driver machine and not needed on any of the worker machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spark-nlp==4.2.1\n",
      "  Downloading spark_nlp-4.2.1-py2.py3-none-any.whl (643 kB)\n",
      "\u001b[K     |████████████████████████████████| 643 kB 30.0 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: spark-nlp\n",
      "  Attempting uninstall: spark-nlp\n",
      "    Found existing installation: spark-nlp 4.2.2\n",
      "    Uninstalling spark-nlp-4.2.2:\n",
      "      Successfully uninstalled spark-nlp-4.2.2\n",
      "Successfully installed spark-nlp-4.2.1\n",
      "Collecting sparknlp\n",
      "  Downloading sparknlp-1.0.0-py3-none-any.whl (1.4 kB)\n",
      "Requirement already satisfied: numpy in /mnt/miniconda/lib/python3.7/site-packages (from sparknlp) (1.21.2)\n",
      "Requirement already satisfied: spark-nlp in /mnt/miniconda/lib/python3.7/site-packages (from sparknlp) (4.2.1)\n",
      "Installing collected packages: sparknlp\n",
      "Successfully installed sparknlp-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!/mnt/miniconda/bin/pip install spark-nlp==4.2.1 --force\n",
    "!/mnt/miniconda/bin/pip install sparknlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yMmT9S6mE0ad"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp\n",
    "from sparknlp.pretrained import PretrainedPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3n4NloINS22"
   },
   "source": [
    "## 2. Start Spark Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that we are calling a specific Java package to connect to the Spark NLP package.\n",
    "\n",
    "We are using [Kryo syrialization](https://spark.apache.org/docs/latest/tuning.html), which is a faster but less flexible method of serializing data to move between processes.\n",
    "\n",
    "Note that this command will download and install MANY Mazen repo Java package dependencies for the Spark-NLP package. It will take a minute or two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/hadoop/.ivy2/cache\n",
      "The jars for the packages stored in: /home/hadoop/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/lib/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-97361bbb-25ef-4e83-9f49-e9888d90ed03;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;4.2.1 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.828 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.code.findbugs#annotations;3.0.1 in central\n",
      "\tfound net.jcip#jcip-annotations;1.0 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.21 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.3 in central\n",
      "downloading https://repo1.maven.org/maven2/com/johnsnowlabs/nlp/spark-nlp_2.12/4.2.1/spark-nlp_2.12-4.2.1.jar ...\n",
      "\t[SUCCESSFUL ] com.johnsnowlabs.nlp#spark-nlp_2.12;4.2.1!spark-nlp_2.12.jar (755ms)\n",
      "downloading https://repo1.maven.org/maven2/com/typesafe/config/1.4.2/config-1.4.2.jar ...\n",
      "\t[SUCCESSFUL ] com.typesafe#config;1.4.2!config.jar(bundle) (5ms)\n",
      "downloading https://repo1.maven.org/maven2/org/rocksdb/rocksdbjni/6.29.5/rocksdbjni-6.29.5.jar ...\n",
      "\t[SUCCESSFUL ] org.rocksdb#rocksdbjni;6.29.5!rocksdbjni.jar (451ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.828/aws-java-sdk-bundle-1.11.828.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.11.828!aws-java-sdk-bundle.jar (1249ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/universal-automata/liblevenshtein/3.0.0/liblevenshtein-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] com.github.universal-automata#liblevenshtein;3.0.0!liblevenshtein.jar (6ms)\n",
      "downloading https://repo1.maven.org/maven2/com/navigamez/greex/1.0/greex-1.0.jar ...\n",
      "\t[SUCCESSFUL ] com.navigamez#greex;1.0!greex.jar (3ms)\n",
      "downloading https://repo1.maven.org/maven2/com/johnsnowlabs/nlp/tensorflow-cpu_2.12/0.4.3/tensorflow-cpu_2.12-0.4.3.jar ...\n",
      "\t[SUCCESSFUL ] com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.3!tensorflow-cpu_2.12.jar (1755ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/annotations/3.0.1/annotations-3.0.1.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#annotations;3.0.1!annotations.jar (3ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/protobuf/protobuf-java-util/3.0.0-beta-3/protobuf-java-util-3.0.0-beta-3.jar ...\n",
      "\t[SUCCESSFUL ] com.google.protobuf#protobuf-java-util;3.0.0-beta-3!protobuf-java-util.jar(bundle) (3ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/protobuf/protobuf-java/3.0.0-beta-3/protobuf-java-3.0.0-beta-3.jar ...\n",
      "\t[SUCCESSFUL ] com.google.protobuf#protobuf-java;3.0.0-beta-3!protobuf-java.jar(bundle) (13ms)\n",
      "downloading https://repo1.maven.org/maven2/it/unimi/dsi/fastutil/7.0.12/fastutil-7.0.12.jar ...\n",
      "\t[SUCCESSFUL ] it.unimi.dsi#fastutil;7.0.12!fastutil.jar (116ms)\n",
      "downloading https://repo1.maven.org/maven2/org/projectlombok/lombok/1.16.8/lombok-1.16.8.jar ...\n",
      "\t[SUCCESSFUL ] org.projectlombok#lombok;1.16.8!lombok.jar (13ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.21/slf4j-api-1.7.21.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.21!slf4j-api.jar (3ms)\n",
      "downloading https://repo1.maven.org/maven2/net/jcip/jcip-annotations/1.0/jcip-annotations-1.0.jar ...\n",
      "\t[SUCCESSFUL ] net.jcip#jcip-annotations;1.0!jcip-annotations.jar (2ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.1/jsr305-3.0.1.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.1!jsr305.jar (3ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/gson/gson/2.3/gson-2.3.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.gson#gson;2.3!gson.jar (4ms)\n",
      "downloading https://repo1.maven.org/maven2/dk/brics/automaton/automaton/1.11-8/automaton-1.11-8.jar ...\n",
      "\t[SUCCESSFUL ] dk.brics.automaton#automaton;1.11-8!automaton.jar (4ms)\n",
      ":: resolution report :: resolve 2752ms :: artifacts dl 4399ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.828 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.code.findbugs#annotations;3.0.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.1 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;4.2.1 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.3 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tnet.jcip#jcip-annotations;1.0 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.21 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   17  |   17  |   17  |   0   ||   17  |   17  |\n",
      "\t---------------------------------------------------------------------\n",
      "\n",
      ":: problems summary ::\n",
      ":::: ERRORS\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/com/amazonaws/aws-java-sdk-pom/1.11.828/aws-java-sdk-pom-1.11.828.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/sonatype/oss/oss-parent/7/oss-parent-7.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/net/jcip/jcip-annotations/1.0/jcip-annotations-1.0-javadoc.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/com/google/google/1/google-1.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/com/google/protobuf/protobuf-parent/3.0.0-beta-3/protobuf-parent-3.0.0-beta-3.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/com/google/protobuf/protobuf-java-util/3.0.0-beta-3/protobuf-java-util-3.0.0-beta-3-sources.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/com/google/protobuf/protobuf-java-util/3.0.0-beta-3/protobuf-java-util-3.0.0-beta-3-src.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/com/google/protobuf/protobuf-java-util/3.0.0-beta-3/protobuf-java-util-3.0.0-beta-3-javadoc.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/com/google/protobuf/protobuf-java/3.0.0-beta-3/protobuf-java-3.0.0-beta-3-sources.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/com/google/protobuf/protobuf-java/3.0.0-beta-3/protobuf-java-3.0.0-beta-3-src.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/com/google/protobuf/protobuf-java/3.0.0-beta-3/protobuf-java-3.0.0-beta-3-javadoc.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/sonatype/oss/oss-parent/9/oss-parent-9.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/slf4j/slf4j-parent/1.7.21/slf4j-parent-1.7.21.jar\n",
      "\n",
      "\n",
      ":: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-97361bbb-25ef-4e83-9f49-e9888d90ed03\n",
      "\tconfs: [default]\n",
      "\t17 artifacts copied, 0 already retrieved (574487kB/360ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/10/31 19:49:03 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "22/10/31 19:49:09 WARN Client: Same path resource file:///home/hadoop/.ivy2/jars/com.johnsnowlabs.nlp_spark-nlp_2.12-4.2.1.jar added multiple times to distributed cache.\n",
      "22/10/31 19:49:09 WARN Client: Same path resource file:///home/hadoop/.ivy2/jars/com.typesafe_config-1.4.2.jar added multiple times to distributed cache.\n",
      "22/10/31 19:49:09 WARN Client: Same path resource file:///home/hadoop/.ivy2/jars/org.rocksdb_rocksdbjni-6.29.5.jar added multiple times to distributed cache.\n",
      "22/10/31 19:49:09 WARN Client: Same path resource file:///home/hadoop/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.828.jar added multiple times to distributed cache.\n",
      "22/10/31 19:49:09 WARN Client: Same path resource file:///home/hadoop/.ivy2/jars/com.github.universal-automata_liblevenshtein-3.0.0.jar added multiple times to distributed cache.\n",
      "22/10/31 19:49:09 WARN Client: Same path resource file:///home/hadoop/.ivy2/jars/com.navigamez_greex-1.0.jar added multiple times to distributed cache.\n",
      "22/10/31 19:49:09 WARN Client: Same path resource file:///home/hadoop/.ivy2/jars/com.johnsnowlabs.nlp_tensorflow-cpu_2.12-0.4.3.jar added multiple times to distributed cache.\n",
      "22/10/31 19:49:09 WARN Client: Same path resource file:///home/hadoop/.ivy2/jars/com.google.code.findbugs_annotations-3.0.1.jar added multiple times to distributed cache.\n",
      "22/10/31 19:49:09 WARN Client: Same path resource file:///home/hadoop/.ivy2/jars/com.google.protobuf_protobuf-java-util-3.0.0-beta-3.jar added multiple times to distributed cache.\n",
      "22/10/31 19:49:09 WARN Client: Same path resource file:///home/hadoop/.ivy2/jars/com.google.protobuf_protobuf-java-3.0.0-beta-3.jar added multiple times to distributed cache.\n",
      "22/10/31 19:49:09 WARN Client: Same path resource file:///home/hadoop/.ivy2/jars/it.unimi.dsi_fastutil-7.0.12.jar added multiple times to distributed cache.\n",
      "22/10/31 19:49:09 WARN Client: Same path resource file:///home/hadoop/.ivy2/jars/org.projectlombok_lombok-1.16.8.jar added multiple times to distributed cache.\n",
      "22/10/31 19:49:09 WARN Client: Same path resource file:///home/hadoop/.ivy2/jars/org.slf4j_slf4j-api-1.7.21.jar added multiple times to distributed cache.\n",
      "22/10/31 19:49:09 WARN Client: Same path resource file:///home/hadoop/.ivy2/jars/net.jcip_jcip-annotations-1.0.jar added multiple times to distributed cache.\n",
      "22/10/31 19:49:09 WARN Client: Same path resource file:///home/hadoop/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.1.jar added multiple times to distributed cache.\n",
      "22/10/31 19:49:09 WARN Client: Same path resource file:///home/hadoop/.ivy2/jars/com.google.code.gson_gson-2.3.jar added multiple times to distributed cache.\n",
      "22/10/31 19:49:09 WARN Client: Same path resource file:///home/hadoop/.ivy2/jars/dk.brics.automaton_automaton-1.11-8.jar added multiple times to distributed cache.\n",
      "22/10/31 19:49:17 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName(\"SparkNLP\") \\\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "        .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:4.2.1\") \\\n",
    "    .master('yarn') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-38-22.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0-amzn-0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkNLP</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe2c5447050>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4--UYjmMNWBK"
   },
   "source": [
    "## 3. Select the DL model and re-run cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1XxHWemdE5hX"
   },
   "outputs": [],
   "source": [
    "#MODEL_NAME='sentimentdl_use_imdb'\n",
    "MODEL_NAME='sentimentdl_use_twitter'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tp7MDziuNdGa"
   },
   "source": [
    "## 4. Some sample examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "GJ7GCD0pFDvP"
   },
   "outputs": [],
   "source": [
    "## Generating Example Files ##\n",
    "\n",
    "text_list = []\n",
    "if MODEL_NAME=='sentimentdl_use_imdb':\n",
    "  text_list = [\n",
    "             \"\"\"Demonicus is a movie turned into a video game! I just love the story and the things that goes on in the film.It is a B-film ofcourse but that doesn`t bother one bit because its made just right and the music was rad! Horror and sword fight freaks,buy this movie now!\"\"\",\n",
    "             \"\"\"Back when Alec Baldwin and Kim Basinger were a mercurial, hot-tempered, high-powered Hollywood couple they filmed this (nearly) scene-for-scene remake of the 1972 Steve McQueen-Ali MacGraw action-thriller about a fugitive twosome. It almost worked the first time because McQueen was such a vital presence on the screen--even stone silent and weary, you could sense his clock ticking, his cagey magnetism. Baldwin is not in Steve McQueen's league, but he has his charms and is probably a more versatile actor--if so, this is not a showcase for his attributes. Basinger does well and certainly looks good, but James Woods is artificially hammy in a silly mob-magnet role. A sub-plot involving another couple taken hostage by Baldwin's ex-partner was unbearable in the '72 film and plays even worse here. As for the action scenes, they're pretty old hat, which causes one to wonder: why even remake the original?\"\"\",\n",
    "             \"\"\"Despite a tight narrative, Johnnie To's Election feels at times like it was once a longer picture, with many characters and plot strands abandoned or ultimately unresolved. Some of these are dealt with in the truly excellent and far superior sequel, Election 2: Harmony is a Virtue, but it's still a dependably enthralling thriller about a contested Triad election that bypasses the usual shootouts and explosions (though not the violence) in favour of constantly shifting alliances that can turn in the time it takes to make a phone call. It's also a film where the most ruthless character isn't always the most threatening one, as the chilling ending makes only too clear: one can imagine a lifetime of psychological counselling being necessary for all the trauma that one inflicts on one unfortunate bystander. Simon Yam, all too often a variable actor but always at his best under To's direction, has possibly never been better in the lead, not least because Tony Leung's much more extrovert performance makes his stillness more the powerful.\"\"\",\n",
    "             \"\"\"This movie has successfully proved what we all already know, that professional basket-ball players suck at everything besides playing basket-ball. Especially rapping and acting. I can not even begin to describe how bad this movie truly is. First of all, is it just me, or is that the ugliest kid you have ever seen? I mean, his teeth could be used as a can-opener. Secondly, why would a genie want to pursue a career in the music industry when, even though he has magical powers, he sucks horribly at making music? Third, I have read the Bible. In no way shape or form did it say that Jesus made genies. Fourth, what was the deal with all the crappy special effects? I assure you that any acne-addled nerdy teenager with a computer could make better effects than that. Fifth, why did the ending suck so badly? And what the hell is a djin? And finally, whoever created the nightmare known as Kazaam needs to be thrown off of a plane and onto the Eiffel Tower, because this movie take the word \"suck\" to an entirely new level.\"\"\",\n",
    "             \"\"\"The fluttering of butterfly wings in the Atlantic can unleash a hurricane in the Pacific. According to this theory (somehow related to the Chaos Theory, I'm not sure exactly how), every action, no matter how small or insignificant, will start a chain reaction that can lead to big events. This small jewel of a film shows us a series of seemingly-unrelated characters, most of them in Paris, whose actions will affect each others' lives. (The six-degrees-of-separation theory can be applied as well.) Each story is a facet of the jewel that is this film. The acting is finely-tuned and nuanced (Audrey Tautou is luminous), the stories mesh plausibly, the humor is just right, and the viewer leaves the theatre nodding in agreement.\"\"\",\n",
    "             \"\"\"There have been very few films I have not been able to sit through. I made it through Battle Field Earth no problem. But this, This is one of the single worst films EVER to be made. I understand Whoopi Goldberg tried to get of acting in it. I do not blame her. I would feel ashamed to have this on a resume. I belive it is a rare occasion when almost every gag in a film falls flat on it's face. Well it happens here. Not to mention the SFX, look for the dino with the control cables hanging out of it rear end!!!!!! Halfway through the film I was still looking for a plot. I never found one. Save yourself the trouble of renting this and save 90 minutes of your life.\"\"\",\n",
    "             \"\"\"After a long hard week behind the desk making all those dam serious decisions this movie is a great way to relax. Like Wells and the original radio broadcast this movie will take you away to a land of alien humor and sci-fi paraday. 'Captain Zippo died in the great charge of the Buick. He was a brave man.' The Jack Nicholson impressions shine right through that alien face with the dark sun glasses and leather jacket. And always remember to beware of the 'doughnut of death!' Keep in mind the number one rule of this movie - suspension of disbelief - sit back and relax - and 'Prepare to die Earth Scum!' You just have to see it for yourself.\"\"\",\n",
    "             \"\"\"When Ritchie first burst on to movie scene his films were hailed as funny, witty, well directed and original. If one could compare the hype he had generated with his first two attempts and the almost universal loathing his last two outings have created one should consider - has Ritchie been found out? Is he really that talented? Does he really have any genuine original ideas? Or is he simply a pretentious and egotistical director who really wants to be Fincher, Tarantino and Leone all rolled into one colossal and disorganised heap? After watching Revolver one could be excused for thinking were did it all go wrong? What happened to his great sense of humour? Where did he get all these mixed and convoluted ideas from? Revolver tries to be clever, philosophical and succinct, it tries to be an intelligent psychoanalysis, it tries to be an intricate and complicated thriller. Ritchie does make a gargantuan effort to fulfil all these many objectives and invests great chunks of a script into existential musings and numerous plot twists. However, in the end all it serves is to construct a severely disjointed, unstructured and ultimately unfriendly film to the audience. Its plagiarism is so sinful and blatant that although Ritchie does at least attempt to give his own spin he should be punished for even trying to pass it off as his own work. So what the audience gets ultimately is a terrible screenplay intertwined with many pretentious oneliners and clumsy setpieces.<br /><br />Revolver is ultimately an unoriginal and bland movie that has stolen countless themes from masterpieces like Fight Club, Usual Suspects and Pulp Fiction. It aims high, but inevitably shots blanks aplenty.<br /><br />Revolver deserves to be lambasted, it is a truly poor film masquerading as a wannabe masterpiece from a wannabe auteur. However, it falls flat on its farcical face and just fails at everything it wants to be and achieve.\"\"\",\n",
    "             \"\"\"I always thought this would be a long and boring Talking-Heads flick full of static interior takes, dude, I was wrong. \"Election\" is a highly fascinating and thoroughly captivating thriller-drama, taking a deep and realistic view behind the origins of Triads-Rituals. Characters are constantly on the move, and although as a viewer you kinda always remain an outsider, it\\'s still possible to feel the suspense coming from certain decisions and ambitions of the characters. Furthermore Johnnie To succeeds in creating some truly opulent images due to meticulously composed lighting and atmospheric light-shadow contrasts. Although there\\'s hardly any action, the ending is still shocking in it\\'s ruthless depicting of brutality. Cool movie that deserves more attention, and I came to like the minimalistic acoustic guitar score quite a bit.\"\"\",\n",
    "             \"\"\"This is to the Zatoichi movies as the \"Star Trek\" movies were to \"Star Trek\"--except that in this case every one of the originals was more entertaining and interesting than this big, shiny re-do, and also better made, if substance is more important than surface. Had I never seen them, I would have thought this good-looking but empty; since I had, I thought its style inappropriate and its content insufficient. The idea of reviving the character in a bigger, slicker production must have sounded good, but there was no point in it, other than the hope of making money; it\\'s just a show, which mostly fails to capture the atmosphere of the character\\'s world and wholly fails to take the character anywhere he hasn\\'t been already (also, the actor wasn\\'t at his best). I\\'d been hoping to see Ichi at a late stage of life, in a story that would see him out gracefully and draw some conclusion from his experience overall; this just rehashes bits and pieces from the other movies, seasoned with more sex and sfx violence. Not the same experience at all.\"\"\"\n",
    "             ]\n",
    "elif  MODEL_NAME=='sentimentdl_use_twitter':\n",
    "  text_list = [\n",
    "            \"\"\"@Mbjthegreat i really dont want AT&amp;T phone service..they suck when it comes to having a signal\"\"\",\n",
    "            \"\"\"holy crap. I take a nap for 4 hours and Pitchfork blows up my twitter dashboard. I wish I was at Coachella.\"\"\",\n",
    "            \"\"\"@Susy412 he is working today  ive tried that still not working..... hmmmm!! im rubbish with computers haha!\"\"\",\n",
    "            \"\"\"Brand New Canon EOS 50D 15MP DSLR Camera Canon 17-85mm IS Lens ...: Web Technology Thread, Brand New Canon EOS 5.. http://u.mavrev.com/5a3t\"\"\",\n",
    "            \"\"\"Watching a programme about the life of Hitler, its only enhancing my geekiness of history.\"\"\",\n",
    "            \"\"\"GM says expects announcment on sale of Hummer soon - Reuters: WDSUGM says expects announcment on sale of Hummer .. http://bit.ly/4E1Fv\"\"\",\n",
    "            \"\"\"@accannis @edog1203 Great Stanford course. Thanks for making it available to the public! Really helpful and informative for starting off!\"\"\",\n",
    "            \"\"\"@the_real_usher LeBron is cool.  I like his personality...he has good character.\"\"\",\n",
    "            \"\"\"@sketchbug Lebron is a hometown hero to me, lol I love the Lakers but let's go Cavs, lol\"\"\",\n",
    "            \"\"\"@PDubyaD right!!! LOL we'll get there!! I have high expectations, Warren Buffet style.\"\"\",\n",
    "            ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sl8vsy03Np6C"
   },
   "source": [
    "## 5. Define Spark NLP pipleline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IiYxv0mOFIcX",
    "outputId": "16b247d9-c8f6-45d2-fcdf-3f5fdfdf069f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfhub_use download started this may take some time.\n",
      "Approximate size to download 923.7 MB\n",
      "[ | ]tfhub_use download started this may take some time.\n",
      "Approximate size to download 923.7 MB\n",
      "[ \\ ]Download done! Loading the resource.\n",
      "[ | ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ / ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ — ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-31 19:50:10.528418: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ | ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-31 19:50:15.140513: W external/org_tensorflow/tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 60236800 exceeds 10% of free system memory.\n",
      "2022-10-31 19:50:15.187226: W external/org_tensorflow/tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 60236800 exceeds 10% of free system memory.\n",
      "2022-10-31 19:50:15.234789: W external/org_tensorflow/tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 60236800 exceeds 10% of free system memory.\n",
      "2022-10-31 19:50:15.313565: W external/org_tensorflow/tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 60236800 exceeds 10% of free system memory.\n",
      "2022-10-31 19:50:15.362942: W external/org_tensorflow/tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 60236800 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n",
      "sentimentdl_use_twitter download started this may take some time.\n",
      "Approximate size to download 11.4 MB\n",
      "[ | ]sentimentdl_use_twitter download started this may take some time.\n",
      "Approximate size to download 11.4 MB\n",
      "[ / ]Download done! Loading the resource.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "    \n",
    "use = UniversalSentenceEncoder.pretrained(name=\"tfhub_use\", lang=\"en\")\\\n",
    " .setInputCols([\"document\"])\\\n",
    " .setOutputCol(\"sentence_embeddings\")\n",
    "\n",
    "\n",
    "sentimentdl = SentimentDLModel.pretrained(name=MODEL_NAME, lang=\"en\")\\\n",
    "    .setInputCols([\"sentence_embeddings\"])\\\n",
    "    .setOutputCol(\"sentiment\")\n",
    "\n",
    "nlpPipeline = Pipeline(\n",
    "      stages = [\n",
    "          documentAssembler,\n",
    "          use,\n",
    "          sentimentdl\n",
    "      ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rFCvvHnN3ox"
   },
   "source": [
    "## 6. Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "fu9l7NI4N53g"
   },
   "outputs": [],
   "source": [
    "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
    "pipelineModel = nlpPipeline.fit(empty_df)\n",
    "\n",
    "df = spark.createDataFrame(pd.DataFrame({\"text\":text_list}))\n",
    "result = pipelineModel.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OusXsM-BN2LF"
   },
   "source": [
    "## 7. Visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the schema and show some rows of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- document: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |-- sentence_embeddings: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |-- sentiment: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document| sentence_embeddings|           sentiment|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|@Mbjthegreat i re...|[[document, 0, 97...|[[sentence_embedd...|[[category, 0, 97...|\n",
      "|holy crap. I take...|[[document, 0, 10...|[[sentence_embedd...|[[category, 0, 10...|\n",
      "|@Susy412 he is wo...|[[document, 0, 10...|[[sentence_embedd...|[[category, 0, 10...|\n",
      "|Brand New Canon E...|[[document, 0, 13...|[[sentence_embedd...|[[category, 0, 13...|\n",
      "|Watching a progra...|[[document, 0, 89...|[[sentence_embedd...|[[category, 0, 89...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this data is so much more complex, we need to pull out just the pieces we care about: the text and the sentiment result. This cell does that for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------+--------+\n",
      "|text                                                                                                                                       |col     |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------+--------+\n",
      "|@Mbjthegreat i really dont want AT&amp;T phone service..they suck when it comes to having a signal                                         |negative|\n",
      "|holy crap. I take a nap for 4 hours and Pitchfork blows up my twitter dashboard. I wish I was at Coachella.                                |negative|\n",
      "|@Susy412 he is working today  ive tried that still not working..... hmmmm!! im rubbish with computers haha!                                |negative|\n",
      "|Brand New Canon EOS 50D 15MP DSLR Camera Canon 17-85mm IS Lens ...: Web Technology Thread, Brand New Canon EOS 5.. http://u.mavrev.com/5a3t|negative|\n",
      "|Watching a programme about the life of Hitler, its only enhancing my geekiness of history.                                                 |negative|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.select('text', F.explode('sentiment.result')).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Sentiment Model for News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using data from a news summarization dataset from [Kaggle](https://www.kaggle.com/datasets/sbhatti/news-summarization). The data has been converted from a zipped csv file into a multi-part parquet file. Load the data into your environment by following the standard steps:\n",
    "    \n",
    "- Copy data from central bucket to personal bucket\n",
    "- Read in data to Spark from personal bucket\n",
    "- Review the structure, size, number of partitions, and show a few rows of data\n",
    "- Create a `df_small` dataset that contains a sample of the full data using the `sample` [function](https://sparkbyexamples.com/pyspark/pyspark-sampling-example/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-31 19:52:09,150 INFO tools.DistCp: Input Options: DistCpOptions{atomicCommit=false, syncFolder=false, deleteMissing=false, ignoreFailures=false, overwrite=false, append=false, useDiff=false, useRdiff=false, fromSnapshot=null, toSnapshot=null, skipCRC=false, blocking=true, numListstatusThreads=0, maxMaps=20, mapBandwidth=0.0, copyStrategy='uniformsize', preserveStatus=[BLOCKSIZE], atomicWorkPath=null, logPath=null, sourceFileListing=null, sourcePaths=[s3://bigdatateaching/news/summarization], targetPath=s3://anly502-fall-2022-yl1353/news/summarization, filtersFile='null', blocksPerChunk=0, copyBufferSize=8192, verboseLog=false, directWrite=false}, sourcePaths=[s3://bigdatateaching/news/summarization], targetPathExists=true, preserveRawXattrsfalse\n",
      "2022-10-31 19:52:09,530 INFO client.RMProxy: Connecting to ResourceManager at ip-172-31-38-22.ec2.internal/172.31.38.22:8032\n",
      "2022-10-31 19:52:09,874 INFO client.AHSProxy: Connecting to Application History server at ip-172-31-38-22.ec2.internal/172.31.38.22:10200\n",
      "2022-10-31 19:52:11,247 INFO tools.SimpleCopyListing: Paths (files+dirs) cnt = 21; dirCnt = 1\n",
      "2022-10-31 19:52:11,248 INFO tools.SimpleCopyListing: Build file listing completed.\n",
      "2022-10-31 19:52:11,249 INFO Configuration.deprecation: io.sort.mb is deprecated. Instead, use mapreduce.task.io.sort.mb\n",
      "2022-10-31 19:52:11,252 INFO Configuration.deprecation: io.sort.factor is deprecated. Instead, use mapreduce.task.io.sort.factor\n",
      "2022-10-31 19:52:11,358 INFO tools.DistCp: Number of paths in the copy list: 21\n",
      "2022-10-31 19:52:11,411 INFO tools.DistCp: Number of paths in the copy list: 21\n",
      "2022-10-31 19:52:11,428 INFO client.RMProxy: Connecting to ResourceManager at ip-172-31-38-22.ec2.internal/172.31.38.22:8032\n",
      "2022-10-31 19:52:11,429 INFO client.AHSProxy: Connecting to Application History server at ip-172-31-38-22.ec2.internal/172.31.38.22:10200\n",
      "2022-10-31 19:52:11,523 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1667240456160_0002\n",
      "2022-10-31 19:52:11,728 INFO mapreduce.JobSubmitter: number of splits:20\n",
      "2022-10-31 19:52:12,002 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1667240456160_0002\n",
      "2022-10-31 19:52:12,004 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2022-10-31 19:52:12,231 INFO conf.Configuration: resource-types.xml not found\n",
      "2022-10-31 19:52:12,232 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2022-10-31 19:52:12,314 INFO impl.YarnClientImpl: Submitted application application_1667240456160_0002\n",
      "2022-10-31 19:52:12,375 INFO mapreduce.Job: The url to track the job: http://ip-172-31-38-22.ec2.internal:20888/proxy/application_1667240456160_0002/\n",
      "2022-10-31 19:52:12,375 INFO tools.DistCp: DistCp job-id: job_1667240456160_0002\n",
      "2022-10-31 19:52:12,377 INFO mapreduce.Job: Running job: job_1667240456160_0002\n",
      "2022-10-31 19:52:17,914 INFO mapreduce.Job: Job job_1667240456160_0002 running in uber mode : false\n",
      "2022-10-31 19:52:17,915 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2022-10-31 19:52:29,010 INFO mapreduce.Job:  map 10% reduce 0%\n",
      "2022-10-31 19:52:35,039 INFO mapreduce.Job:  map 70% reduce 0%\n",
      "2022-10-31 19:52:36,045 INFO mapreduce.Job:  map 95% reduce 0%\n",
      "2022-10-31 19:52:37,049 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2022-10-31 19:52:39,062 INFO mapreduce.Job: Job job_1667240456160_0002 completed successfully\n",
      "2022-10-31 19:52:39,156 INFO mapreduce.Job: Counters: 42\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=4832806\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=12384\n",
      "\t\tHDFS: Number of bytes written=1225\n",
      "\t\tHDFS: Number of read operations=160\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=40\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tS3: Number of bytes read=0\n",
      "\t\tS3: Number of bytes written=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=20\n",
      "\t\tOther local map tasks=20\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=22145184\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=230679\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=230679\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=708645888\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=21\n",
      "\t\tMap output records=20\n",
      "\t\tInput split bytes=2720\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=6876\n",
      "\t\tCPU time spent (ms)=131660\n",
      "\t\tPhysical memory (bytes) snapshot=8969424896\n",
      "\t\tVirtual memory (bytes) snapshot=89228537856\n",
      "\t\tTotal committed heap usage (bytes)=8228175872\n",
      "\t\tPeak Map Physical memory (bytes)=565780480\n",
      "\t\tPeak Map Virtual memory (bytes)=4488654848\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=9664\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1225\n",
      "\tDistCp Counters\n",
      "\t\tBandwidth in Btyes=0\n",
      "\t\tBytes Skipped=3825232272\n",
      "\t\tDIR_COPY=1\n",
      "\t\tFiles Skipped=20\n"
     ]
    }
   ],
   "source": [
    "!hadoop distcp s3://bigdatateaching/news/summarization/ s3://anly502-fall-2022-yl1353/news/summarization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet('s3://anly502-fall-2022-yl1353/news/summarization/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "870521"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------+-------------------+\n",
      "|                  ID|             Content|             Summary|       Dataset|__null_dask_index__|\n",
      "+--------------------+--------------------+--------------------+--------------+-------------------+\n",
      "|f49ee725a0360aa68...|New York police a...|Police have inves...|CNN/Daily Mail|                  0|\n",
      "|808fe317a53fbd313...|By . Ryan Lipman ...|Porn star Angela ...|CNN/Daily Mail|                  1|\n",
      "|98fd67bd343e58bc4...|This was, Sergio ...|American draws in...|CNN/Daily Mail|                  2|\n",
      "|e12b5bd7056287049...|An Ebola outbreak...|World Health Orga...|CNN/Daily Mail|                  3|\n",
      "|b83e8bcfcd5141984...|By . Associated P...|A sinkhole opened...|CNN/Daily Mail|                  4|\n",
      "+--------------------+--------------------+--------------------+--------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = df.sample(withReplacement = True, fraction = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------+-------------------+\n",
      "|                  ID|             Content|             Summary|       Dataset|__null_dask_index__|\n",
      "+--------------------+--------------------+--------------------+--------------+-------------------+\n",
      "|550c7ea14b4ec91db...|An Australian fat...|Zia Abdul Haq is ...|CNN/Daily Mail|                  6|\n",
      "|85fa186e116866297...|A community stalw...|Rahmat Ali Raja, ...|CNN/Daily Mail|                  8|\n",
      "|6c9ecd04c8b2bd960...|(CNN) -- Three De...|Police arrested T...|CNN/Daily Mail|                 13|\n",
      "|872dc5f31eedcb249...|Queens Park Range...|Hull and Stoke ar...|CNN/Daily Mail|                 16|\n",
      "|b8e14b9ce93a52020...|Scathing: A repor...|Sir Cliff first s...|CNN/Daily Mail|                 17|\n",
      "+--------------------+--------------------+--------------------+--------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_small.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct four dummy variables using pyspark and the regex function `rlike`. You will make each dummy variable using the regex statement provided below:\n",
    "\n",
    "|dummy | regex|\n",
    "|-----------|-----------|\n",
    "|politics|**(?i)politics\\|(?i)political\\|(?i)senate\\|(?i)government\\|(?i)president\\|(?i)prime minister\\|(?i)congress**|\n",
    "|sports|**(?i)sport\\|(?i)ball\\|(?i)coach\\|(?i)goal\\|(?i)baseball\\|(?i)football\\|(?i)basketball**|\n",
    "|arts|**(?i)art\\|(?i)painting\\|(?i)artist\\|(?i)museum\\|(?i)photography\\|(?i)sculpture**|\n",
    "|history|**(?i)history\\|(?i)historical\\|(?i)ancient\\|(?i)archaeology\\|(?i)heritage\\|(?i)fossil**|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"politics\", when(df.Content.rlike(\"(?i)politics|(?i)political|(?i)senate|(?i)government|(?i)president|(?i)prime minister|(?i)congress\"),True)\n",
    "                   .otherwise(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"sports\", when(df.Content.rlike(\"(?i)sport|(?i)ball|(?i)coach|(?i)goal|(?i)baseball|(?i)football|(?i)basketball\"),True)\n",
    "                   .otherwise(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"arts\", when(df.Content.rlike(\"(?i)art|(?i)painting|(?i)artist|(?i)museum|(?i)photography|(?i)sculpture\"),True)\n",
    "                   .otherwise(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"history\", when(df.Content.rlike(\"(?i)history|(?i)historical|(?i)ancient|(?i)archaeology|(?i)heritage|(?i)fossil\"),True)\n",
    "                   .otherwise(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------+-------------------+--------+------+-----+-------+\n",
      "|                  ID|             Content|             Summary|       Dataset|__null_dask_index__|politics|sports| arts|history|\n",
      "+--------------------+--------------------+--------------------+--------------+-------------------+--------+------+-----+-------+\n",
      "|f49ee725a0360aa68...|New York police a...|Police have inves...|CNN/Daily Mail|                  0|   false|  true|false|  false|\n",
      "|808fe317a53fbd313...|By . Ryan Lipman ...|Porn star Angela ...|CNN/Daily Mail|                  1|   false| false| true|  false|\n",
      "|98fd67bd343e58bc4...|This was, Sergio ...|American draws in...|CNN/Daily Mail|                  2|   false|  true| true|  false|\n",
      "|e12b5bd7056287049...|An Ebola outbreak...|World Health Orga...|CNN/Daily Mail|                  3|    true| false| true|  false|\n",
      "|b83e8bcfcd5141984...|By . Associated P...|A sinkhole opened...|CNN/Daily Mail|                  4|   false| false| true|  false|\n",
      "+--------------------+--------------------+--------------------+--------------+-------------------+--------+------+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show counts of each dummy variable in the dataset. Save the result for the dummy count of `arts` to the variable name specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "politics_count = df.groupby(\"politics\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sports_count = df.groupby(\"sports\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "arts_count = df.groupby(\"arts\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_art_soln = arts_count.toPandas().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_count = df.groupby(\"history\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a SparkNLP Pipeline to construct positive/negative sentiment\n",
    "\n",
    "Adapt the previous example to make the sentiment model for this dataset. Remember, start out with a smaller dataset (`df_small`) before moving onto your full dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfhub_use download started this may take some time.\n",
      "Approximate size to download 923.7 MB\n",
      "[OK!]\n",
      "sentimentdl_use_twitter download started this may take some time.\n",
      "Approximate size to download 11.4 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = DocumentAssembler()\\\n",
    "    .setInputCol(\"Content\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "    \n",
    "use = UniversalSentenceEncoder.pretrained(name=\"tfhub_use\", lang=\"en\")\\\n",
    " .setInputCols([\"document\"])\\\n",
    " .setOutputCol(\"sentence_embeddings\")\n",
    "\n",
    "\n",
    "sentimentdl = SentimentDLModel.pretrained(name=MODEL_NAME, lang=\"en\")\\\n",
    "    .setInputCols([\"sentence_embeddings\"])\\\n",
    "    .setOutputCol(\"sentiment\")\n",
    "\n",
    "nlpPipeline = Pipeline(\n",
    "      stages = [\n",
    "          documentAssembler,\n",
    "          use,\n",
    "          sentimentdl\n",
    "      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty_df_s = spark.createDataFrame([['']]).toDF(\"Content\")\n",
    "pipelineModel_df = nlpPipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pipelineModel_df.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|             Content|     col|\n",
      "+--------------------+--------+\n",
      "|New York police a...|negative|\n",
      "|By . Ryan Lipman ...|negative|\n",
      "|This was, Sergio ...|negative|\n",
      "|An Ebola outbreak...|negative|\n",
      "|By . Associated P...|negative|\n",
      "+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sentiment_df = result_df.select('Content', functions.explode('sentiment.result'))\n",
    "sentiment_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull out the sentiment output into its own column in the main dataframe. Create a new dataframe that includes sentiment, news source, and your four dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.join(sentiment_df, df.Content == sentiment_df.Content,\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.drop(sentiment_df.Content)\\\n",
    "                .withColumnRenamed('col','sentiment')\\\n",
    "                .withColumnRenamed('Dataset','news_source')\\\n",
    "                .select('Content','sentiment','news_source','politics','sports','arts','history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Content: string, sentiment: string, news_source: string, politics: boolean, sports: boolean, arts: boolean, history: boolean]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 50:===================================================>    (48 + 4) / 52]\r"
     ]
    }
   ],
   "source": [
    "df_new.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a summary table of the count of articles grouped by your `politics` dummy variable, news source `news_source`, and sentiment classification `sentiment`. Save the resulting dataframe into a variable called `df_sent_baseline`, similar to the previous step for saving the Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_summary = df_new.groupby('politics','news_source','sentiment').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-----+\n",
      "|         news_source|sentiment|count|\n",
      "+--------------------+---------+-----+\n",
      "|\n",
      "  \n",
      " A crowdsourc...| negative|    1|\n",
      "|\"All Americans sh...| negative|    1|\n",
      "|\"I am incredibly ...| negative|    1|\n",
      "|\"I love Ireland,\"...| positive|    1|\n",
      "|\"It cannot be rig...| negative|    1|\n",
      "+--------------------+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_summary.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sent_baseline = table_summary.toPandas().to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Save your analytics results to a json object - then add, commit, and push your notebook and json to GitHub!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dump({'df_arts_count' : df_art_soln,\n",
    "           'df_sentiment_baseline' : df_sent_baseline,\n",
    "          }, \n",
    "          fp = open('lab-soln.json','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STOP YOUR CLUSTER!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SENTIMENT_EN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "45150093197569bb3a58481dcd32cd1adb45462fa3448719e8ac38ada6166aca"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
