{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Spark Machine Learning\n",
    "\n",
    "In this lab you will be digging into the world of credit markets and peer-to-peer lending. Loan decisions from financial institutions depend on a vast trove of data on borrower characteristics, market trends, and more. The best way to harness the vast data is with machine learning. An [article by the Brookings Institution](https://www.brookings.edu/research/credit-denial-in-the-age-of-ai/) discusses the impact of AI on loan decisions and higlights that “If there are data out there on you, there is probably a way to integrate it into a credit model.” "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Spark Machine Learning to build a machine learning pipeline that can model and predict credit worthiness. The data we will be using comes from Lending Club, a company that conducted peer-to-peer lending from 2007 to 2020. There are over 2.9 million loans in the dataset. Each row of data about the loan, credit history, and the success of the loan. Credit history variables include  number of accounts, past missed payments, occupation, years of experience, etc. You can read about the source data [from Kaggle](https://www.kaggle.com/ethon0426/lending-club-20072020q1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all your Spark related environment variables, and pyspark using the `findspark.init()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your SparkSession. You are only going to create a `SparkSession`, not a `SparkContext`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/10/19 00:04:07 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"lendingclub\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure your SparkSession is active:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-44-162.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0-amzn-0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>lendingclub</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff5ac195790>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read in the data\n",
    "\n",
    "The data is in the format of an Apache Parquet file. This file format is optimized for use on clusters, which you can read more about [here](https://drill.apache.org/docs/parquet-format/). You will be working with this file using [Spark DataFrame API and Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) as we did in the SparkSQL lab.\n",
    "\n",
    "Start off by moving the data from the central bucket to your personal bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy: s3://bigdatateaching/lendingclub/lendingclub_loan_data.parquet to s3://anly502-fall-2022-yl1353/lendingclub_loan_data.parquet\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://bigdatateaching/lendingclub s3://anly502-fall-2022-yl1353 --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the Parquet file using [Generic load functions for SparkSQL](https://spark.apache.org/docs/latest/sql-programming-guide.html#data-sources), which is located at `s3://[[YOUR-BUCKET-NAME]]/lendingclub/lendingclub_loan_data.parquet`.\n",
    "\n",
    "Create a DataFrame called `df_in`, which should contain many rows and over 140 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_in = spark.read.parquet('s3://anly502-fall-2022-yl1353/lendingclub_loan_data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever you load in a new dataset, you will always do the following three actions:\n",
    "\n",
    "    1. Get a count of the rows\n",
    "    2. Print the schema of the data\n",
    "    3. View the first few rows of the data (there are a lot of columns so just look at 3)\n",
    "\n",
    "Do these steps now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/19 00:06:07 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2925492"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_in.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- loan_amnt: double (nullable = true)\n",
      " |-- funded_amnt: double (nullable = true)\n",
      " |-- funded_amnt_inv: double (nullable = true)\n",
      " |-- term: string (nullable = true)\n",
      " |-- int_rate: string (nullable = true)\n",
      " |-- installment: double (nullable = true)\n",
      " |-- grade: string (nullable = true)\n",
      " |-- sub_grade: string (nullable = true)\n",
      " |-- emp_title: string (nullable = true)\n",
      " |-- emp_length: string (nullable = true)\n",
      " |-- home_ownership: string (nullable = true)\n",
      " |-- annual_inc: double (nullable = true)\n",
      " |-- verification_status: string (nullable = true)\n",
      " |-- issue_d: string (nullable = true)\n",
      " |-- loan_status: string (nullable = true)\n",
      " |-- pymnt_plan: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- purpose: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- zip_code: string (nullable = true)\n",
      " |-- addr_state: string (nullable = true)\n",
      " |-- dti: double (nullable = true)\n",
      " |-- delinq_2yrs: double (nullable = true)\n",
      " |-- earliest_cr_line: string (nullable = true)\n",
      " |-- fico_range_low: double (nullable = true)\n",
      " |-- fico_range_high: double (nullable = true)\n",
      " |-- inq_last_6mths: double (nullable = true)\n",
      " |-- mths_since_last_delinq: double (nullable = true)\n",
      " |-- mths_since_last_record: double (nullable = true)\n",
      " |-- open_acc: double (nullable = true)\n",
      " |-- pub_rec: double (nullable = true)\n",
      " |-- revol_bal: double (nullable = true)\n",
      " |-- revol_util: string (nullable = true)\n",
      " |-- total_acc: double (nullable = true)\n",
      " |-- initial_list_status: string (nullable = true)\n",
      " |-- out_prncp: double (nullable = true)\n",
      " |-- out_prncp_inv: double (nullable = true)\n",
      " |-- total_pymnt: double (nullable = true)\n",
      " |-- total_pymnt_inv: double (nullable = true)\n",
      " |-- total_rec_prncp: double (nullable = true)\n",
      " |-- total_rec_int: double (nullable = true)\n",
      " |-- total_rec_late_fee: double (nullable = true)\n",
      " |-- recoveries: double (nullable = true)\n",
      " |-- collection_recovery_fee: double (nullable = true)\n",
      " |-- last_pymnt_d: string (nullable = true)\n",
      " |-- last_pymnt_amnt: double (nullable = true)\n",
      " |-- next_pymnt_d: string (nullable = true)\n",
      " |-- last_credit_pull_d: string (nullable = true)\n",
      " |-- last_fico_range_high: double (nullable = true)\n",
      " |-- last_fico_range_low: double (nullable = true)\n",
      " |-- collections_12_mths_ex_med: double (nullable = true)\n",
      " |-- mths_since_last_major_derog: double (nullable = true)\n",
      " |-- policy_code: double (nullable = true)\n",
      " |-- application_type: string (nullable = true)\n",
      " |-- annual_inc_joint: double (nullable = true)\n",
      " |-- dti_joint: double (nullable = true)\n",
      " |-- verification_status_joint: string (nullable = true)\n",
      " |-- acc_now_delinq: double (nullable = true)\n",
      " |-- tot_coll_amt: double (nullable = true)\n",
      " |-- tot_cur_bal: double (nullable = true)\n",
      " |-- open_acc_6m: double (nullable = true)\n",
      " |-- open_act_il: double (nullable = true)\n",
      " |-- open_il_12m: double (nullable = true)\n",
      " |-- open_il_24m: double (nullable = true)\n",
      " |-- mths_since_rcnt_il: double (nullable = true)\n",
      " |-- total_bal_il: double (nullable = true)\n",
      " |-- il_util: double (nullable = true)\n",
      " |-- open_rv_12m: double (nullable = true)\n",
      " |-- open_rv_24m: double (nullable = true)\n",
      " |-- max_bal_bc: double (nullable = true)\n",
      " |-- all_util: double (nullable = true)\n",
      " |-- total_rev_hi_lim: double (nullable = true)\n",
      " |-- inq_fi: double (nullable = true)\n",
      " |-- total_cu_tl: double (nullable = true)\n",
      " |-- inq_last_12m: double (nullable = true)\n",
      " |-- acc_open_past_24mths: double (nullable = true)\n",
      " |-- avg_cur_bal: double (nullable = true)\n",
      " |-- bc_open_to_buy: double (nullable = true)\n",
      " |-- bc_util: double (nullable = true)\n",
      " |-- chargeoff_within_12_mths: double (nullable = true)\n",
      " |-- delinq_amnt: double (nullable = true)\n",
      " |-- mo_sin_old_il_acct: double (nullable = true)\n",
      " |-- mo_sin_old_rev_tl_op: double (nullable = true)\n",
      " |-- mo_sin_rcnt_rev_tl_op: double (nullable = true)\n",
      " |-- mo_sin_rcnt_tl: double (nullable = true)\n",
      " |-- mort_acc: double (nullable = true)\n",
      " |-- mths_since_recent_bc: double (nullable = true)\n",
      " |-- mths_since_recent_bc_dlq: double (nullable = true)\n",
      " |-- mths_since_recent_inq: double (nullable = true)\n",
      " |-- mths_since_recent_revol_delinq: double (nullable = true)\n",
      " |-- num_accts_ever_120_pd: double (nullable = true)\n",
      " |-- num_actv_bc_tl: double (nullable = true)\n",
      " |-- num_actv_rev_tl: double (nullable = true)\n",
      " |-- num_bc_sats: double (nullable = true)\n",
      " |-- num_bc_tl: double (nullable = true)\n",
      " |-- num_il_tl: double (nullable = true)\n",
      " |-- num_op_rev_tl: double (nullable = true)\n",
      " |-- num_rev_accts: double (nullable = true)\n",
      " |-- num_rev_tl_bal_gt_0: double (nullable = true)\n",
      " |-- num_sats: double (nullable = true)\n",
      " |-- num_tl_120dpd_2m: double (nullable = true)\n",
      " |-- num_tl_30dpd: double (nullable = true)\n",
      " |-- num_tl_90g_dpd_24m: double (nullable = true)\n",
      " |-- num_tl_op_past_12m: double (nullable = true)\n",
      " |-- pct_tl_nvr_dlq: double (nullable = true)\n",
      " |-- percent_bc_gt_75: double (nullable = true)\n",
      " |-- pub_rec_bankruptcies: double (nullable = true)\n",
      " |-- tax_liens: double (nullable = true)\n",
      " |-- tot_hi_cred_lim: double (nullable = true)\n",
      " |-- total_bal_ex_mort: double (nullable = true)\n",
      " |-- total_bc_limit: double (nullable = true)\n",
      " |-- total_il_high_credit_limit: double (nullable = true)\n",
      " |-- revol_bal_joint: double (nullable = true)\n",
      " |-- sec_app_fico_range_low: double (nullable = true)\n",
      " |-- sec_app_fico_range_high: double (nullable = true)\n",
      " |-- sec_app_earliest_cr_line: string (nullable = true)\n",
      " |-- sec_app_inq_last_6mths: double (nullable = true)\n",
      " |-- sec_app_mort_acc: double (nullable = true)\n",
      " |-- sec_app_open_acc: double (nullable = true)\n",
      " |-- sec_app_revol_util: double (nullable = true)\n",
      " |-- sec_app_open_act_il: double (nullable = true)\n",
      " |-- sec_app_num_rev_accts: double (nullable = true)\n",
      " |-- sec_app_chargeoff_within_12_mths: double (nullable = true)\n",
      " |-- sec_app_collections_12_mths_ex_med: double (nullable = true)\n",
      " |-- hardship_flag: string (nullable = true)\n",
      " |-- hardship_type: string (nullable = true)\n",
      " |-- hardship_reason: string (nullable = true)\n",
      " |-- hardship_status: string (nullable = true)\n",
      " |-- deferral_term: double (nullable = true)\n",
      " |-- hardship_amount: double (nullable = true)\n",
      " |-- hardship_start_date: string (nullable = true)\n",
      " |-- hardship_end_date: string (nullable = true)\n",
      " |-- payment_plan_start_date: string (nullable = true)\n",
      " |-- hardship_length: double (nullable = true)\n",
      " |-- hardship_dpd: double (nullable = true)\n",
      " |-- hardship_loan_status: string (nullable = true)\n",
      " |-- orig_projected_additional_accrued_interest: double (nullable = true)\n",
      " |-- hardship_payoff_balance_amount: double (nullable = true)\n",
      " |-- hardship_last_payment_amount: double (nullable = true)\n",
      " |-- debt_settlement_flag: string (nullable = true)\n",
      " |-- __index_level_0__: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_in.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id='1077501', loan_amnt=5000.0, funded_amnt=5000.0, funded_amnt_inv=4975.0, term=' 36 months', int_rate=' 10.65%', installment=162.87, grade='B', sub_grade='B2', emp_title=None, emp_length='10+ years', home_ownership='RENT', annual_inc=24000.0, verification_status='Verified', issue_d='Dec-2011', loan_status='Fully Paid', pymnt_plan='n', url='https://lendingclub.com/browse/loanDetail.action?loan_id=1077501', purpose='credit_card', title='Computer', zip_code='860xx', addr_state='AZ', dti=27.65, delinq_2yrs=0.0, earliest_cr_line='Jan-1985', fico_range_low=735.0, fico_range_high=739.0, inq_last_6mths=1.0, mths_since_last_delinq=None, mths_since_last_record=None, open_acc=3.0, pub_rec=0.0, revol_bal=13648.0, revol_util='83.7%', total_acc=9.0, initial_list_status='f', out_prncp=0.0, out_prncp_inv=0.0, total_pymnt=5863.1551866952, total_pymnt_inv=5833.84, total_rec_prncp=5000.0, total_rec_int=863.16, total_rec_late_fee=0.0, recoveries=0.0, collection_recovery_fee=0.0, last_pymnt_d='Jan-2015', last_pymnt_amnt=171.62, next_pymnt_d=None, last_credit_pull_d='May-2020', last_fico_range_high=704.0, last_fico_range_low=700.0, collections_12_mths_ex_med=0.0, mths_since_last_major_derog=None, policy_code=1.0, application_type='Individual', annual_inc_joint=None, dti_joint=None, verification_status_joint=None, acc_now_delinq=0.0, tot_coll_amt=None, tot_cur_bal=None, open_acc_6m=None, open_act_il=None, open_il_12m=None, open_il_24m=None, mths_since_rcnt_il=None, total_bal_il=None, il_util=None, open_rv_12m=None, open_rv_24m=None, max_bal_bc=None, all_util=None, total_rev_hi_lim=None, inq_fi=None, total_cu_tl=None, inq_last_12m=None, acc_open_past_24mths=None, avg_cur_bal=None, bc_open_to_buy=None, bc_util=None, chargeoff_within_12_mths=0.0, delinq_amnt=0.0, mo_sin_old_il_acct=None, mo_sin_old_rev_tl_op=None, mo_sin_rcnt_rev_tl_op=None, mo_sin_rcnt_tl=None, mort_acc=None, mths_since_recent_bc=None, mths_since_recent_bc_dlq=None, mths_since_recent_inq=None, mths_since_recent_revol_delinq=None, num_accts_ever_120_pd=None, num_actv_bc_tl=None, num_actv_rev_tl=None, num_bc_sats=None, num_bc_tl=None, num_il_tl=None, num_op_rev_tl=None, num_rev_accts=None, num_rev_tl_bal_gt_0=None, num_sats=None, num_tl_120dpd_2m=None, num_tl_30dpd=None, num_tl_90g_dpd_24m=None, num_tl_op_past_12m=None, pct_tl_nvr_dlq=None, percent_bc_gt_75=None, pub_rec_bankruptcies=0.0, tax_liens=0.0, tot_hi_cred_lim=None, total_bal_ex_mort=None, total_bc_limit=None, total_il_high_credit_limit=None, revol_bal_joint=None, sec_app_fico_range_low=None, sec_app_fico_range_high=None, sec_app_earliest_cr_line=None, sec_app_inq_last_6mths=None, sec_app_mort_acc=None, sec_app_open_acc=None, sec_app_revol_util=None, sec_app_open_act_il=None, sec_app_num_rev_accts=None, sec_app_chargeoff_within_12_mths=None, sec_app_collections_12_mths_ex_med=None, hardship_flag='N', hardship_type=None, hardship_reason=None, hardship_status=None, deferral_term=None, hardship_amount=None, hardship_start_date=None, hardship_end_date=None, payment_plan_start_date=None, hardship_length=None, hardship_dpd=None, hardship_loan_status=None, orig_projected_additional_accrued_interest=None, hardship_payoff_balance_amount=None, hardship_last_payment_amount=None, debt_settlement_flag='N', __index_level_0__=0),\n",
       " Row(id='1077430', loan_amnt=2500.0, funded_amnt=2500.0, funded_amnt_inv=2500.0, term=' 60 months', int_rate=' 15.27%', installment=59.83, grade='C', sub_grade='C4', emp_title='Ryder', emp_length='< 1 year', home_ownership='RENT', annual_inc=30000.0, verification_status='Source Verified', issue_d='Dec-2011', loan_status='Charged Off', pymnt_plan='n', url='https://lendingclub.com/browse/loanDetail.action?loan_id=1077430', purpose='car', title='bike', zip_code='309xx', addr_state='GA', dti=1.0, delinq_2yrs=0.0, earliest_cr_line='Apr-1999', fico_range_low=740.0, fico_range_high=744.0, inq_last_6mths=5.0, mths_since_last_delinq=None, mths_since_last_record=None, open_acc=3.0, pub_rec=0.0, revol_bal=1687.0, revol_util='9.4%', total_acc=4.0, initial_list_status='f', out_prncp=0.0, out_prncp_inv=0.0, total_pymnt=1014.53, total_pymnt_inv=1014.53, total_rec_prncp=456.46, total_rec_int=435.17, total_rec_late_fee=0.0, recoveries=122.9, collection_recovery_fee=1.11, last_pymnt_d='Apr-2013', last_pymnt_amnt=119.66, next_pymnt_d=None, last_credit_pull_d='Oct-2016', last_fico_range_high=499.0, last_fico_range_low=0.0, collections_12_mths_ex_med=0.0, mths_since_last_major_derog=None, policy_code=1.0, application_type='Individual', annual_inc_joint=None, dti_joint=None, verification_status_joint=None, acc_now_delinq=0.0, tot_coll_amt=None, tot_cur_bal=None, open_acc_6m=None, open_act_il=None, open_il_12m=None, open_il_24m=None, mths_since_rcnt_il=None, total_bal_il=None, il_util=None, open_rv_12m=None, open_rv_24m=None, max_bal_bc=None, all_util=None, total_rev_hi_lim=None, inq_fi=None, total_cu_tl=None, inq_last_12m=None, acc_open_past_24mths=None, avg_cur_bal=None, bc_open_to_buy=None, bc_util=None, chargeoff_within_12_mths=0.0, delinq_amnt=0.0, mo_sin_old_il_acct=None, mo_sin_old_rev_tl_op=None, mo_sin_rcnt_rev_tl_op=None, mo_sin_rcnt_tl=None, mort_acc=None, mths_since_recent_bc=None, mths_since_recent_bc_dlq=None, mths_since_recent_inq=None, mths_since_recent_revol_delinq=None, num_accts_ever_120_pd=None, num_actv_bc_tl=None, num_actv_rev_tl=None, num_bc_sats=None, num_bc_tl=None, num_il_tl=None, num_op_rev_tl=None, num_rev_accts=None, num_rev_tl_bal_gt_0=None, num_sats=None, num_tl_120dpd_2m=None, num_tl_30dpd=None, num_tl_90g_dpd_24m=None, num_tl_op_past_12m=None, pct_tl_nvr_dlq=None, percent_bc_gt_75=None, pub_rec_bankruptcies=0.0, tax_liens=0.0, tot_hi_cred_lim=None, total_bal_ex_mort=None, total_bc_limit=None, total_il_high_credit_limit=None, revol_bal_joint=None, sec_app_fico_range_low=None, sec_app_fico_range_high=None, sec_app_earliest_cr_line=None, sec_app_inq_last_6mths=None, sec_app_mort_acc=None, sec_app_open_acc=None, sec_app_revol_util=None, sec_app_open_act_il=None, sec_app_num_rev_accts=None, sec_app_chargeoff_within_12_mths=None, sec_app_collections_12_mths_ex_med=None, hardship_flag='N', hardship_type=None, hardship_reason=None, hardship_status=None, deferral_term=None, hardship_amount=None, hardship_start_date=None, hardship_end_date=None, payment_plan_start_date=None, hardship_length=None, hardship_dpd=None, hardship_loan_status=None, orig_projected_additional_accrued_interest=None, hardship_payoff_balance_amount=None, hardship_last_payment_amount=None, debt_settlement_flag='N', __index_level_0__=1),\n",
       " Row(id='1077175', loan_amnt=2400.0, funded_amnt=2400.0, funded_amnt_inv=2400.0, term=' 36 months', int_rate=' 15.96%', installment=84.33, grade='C', sub_grade='C5', emp_title=None, emp_length='10+ years', home_ownership='RENT', annual_inc=12252.0, verification_status='Not Verified', issue_d='Dec-2011', loan_status='Fully Paid', pymnt_plan='n', url='https://lendingclub.com/browse/loanDetail.action?loan_id=1077175', purpose='small_business', title='real estate business', zip_code='606xx', addr_state='IL', dti=8.72, delinq_2yrs=0.0, earliest_cr_line='Nov-2001', fico_range_low=735.0, fico_range_high=739.0, inq_last_6mths=2.0, mths_since_last_delinq=None, mths_since_last_record=None, open_acc=2.0, pub_rec=0.0, revol_bal=2956.0, revol_util='98.5%', total_acc=10.0, initial_list_status='f', out_prncp=0.0, out_prncp_inv=0.0, total_pymnt=3005.6668441393, total_pymnt_inv=3005.67, total_rec_prncp=2400.0, total_rec_int=605.67, total_rec_late_fee=0.0, recoveries=0.0, collection_recovery_fee=0.0, last_pymnt_d='Jun-2014', last_pymnt_amnt=649.91, next_pymnt_d=None, last_credit_pull_d='Jun-2017', last_fico_range_high=739.0, last_fico_range_low=735.0, collections_12_mths_ex_med=0.0, mths_since_last_major_derog=None, policy_code=1.0, application_type='Individual', annual_inc_joint=None, dti_joint=None, verification_status_joint=None, acc_now_delinq=0.0, tot_coll_amt=None, tot_cur_bal=None, open_acc_6m=None, open_act_il=None, open_il_12m=None, open_il_24m=None, mths_since_rcnt_il=None, total_bal_il=None, il_util=None, open_rv_12m=None, open_rv_24m=None, max_bal_bc=None, all_util=None, total_rev_hi_lim=None, inq_fi=None, total_cu_tl=None, inq_last_12m=None, acc_open_past_24mths=None, avg_cur_bal=None, bc_open_to_buy=None, bc_util=None, chargeoff_within_12_mths=0.0, delinq_amnt=0.0, mo_sin_old_il_acct=None, mo_sin_old_rev_tl_op=None, mo_sin_rcnt_rev_tl_op=None, mo_sin_rcnt_tl=None, mort_acc=None, mths_since_recent_bc=None, mths_since_recent_bc_dlq=None, mths_since_recent_inq=None, mths_since_recent_revol_delinq=None, num_accts_ever_120_pd=None, num_actv_bc_tl=None, num_actv_rev_tl=None, num_bc_sats=None, num_bc_tl=None, num_il_tl=None, num_op_rev_tl=None, num_rev_accts=None, num_rev_tl_bal_gt_0=None, num_sats=None, num_tl_120dpd_2m=None, num_tl_30dpd=None, num_tl_90g_dpd_24m=None, num_tl_op_past_12m=None, pct_tl_nvr_dlq=None, percent_bc_gt_75=None, pub_rec_bankruptcies=0.0, tax_liens=0.0, tot_hi_cred_lim=None, total_bal_ex_mort=None, total_bc_limit=None, total_il_high_credit_limit=None, revol_bal_joint=None, sec_app_fico_range_low=None, sec_app_fico_range_high=None, sec_app_earliest_cr_line=None, sec_app_inq_last_6mths=None, sec_app_mort_acc=None, sec_app_open_acc=None, sec_app_revol_util=None, sec_app_open_act_il=None, sec_app_num_rev_accts=None, sec_app_chargeoff_within_12_mths=None, sec_app_collections_12_mths_ex_med=None, hardship_flag='N', hardship_type=None, hardship_reason=None, hardship_status=None, deferral_term=None, hardship_amount=None, hardship_start_date=None, hardship_end_date=None, payment_plan_start_date=None, hardship_length=None, hardship_dpd=None, hardship_loan_status=None, orig_projected_additional_accrued_interest=None, hardship_payoff_balance_amount=None, hardship_last_payment_amount=None, debt_settlement_flag='N', __index_level_0__=2)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_in.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.1 Clean data to prepare for machine learning\n",
    "\n",
    "No dataset is ever ready for modeling without cleaning. There are several steps required to get the data ready:\n",
    "    \n",
    "1. Select only the columns - `['int_rate', 'term', 'loan_status', 'home_ownership', 'tot_cur_bal', 'annual_inc', 'grade', 'bc_util']` and then drop if there are any coded NA values left `.dropna()` [read more here on the method](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.dropna.html).\n",
    "\n",
    "These columns mean:\n",
    "* `int_rate`: interest rate\n",
    "* `term`: length of the loan\n",
    "* `loan_status`: loan paid, in progress, or charged off\n",
    "* `home_ownership`: home ownership status\n",
    "* `tot_cur_bal`: total current balance on all credit accounts\n",
    "* `grade`: The grade of the loan\n",
    "* `bc_util`: bank card utilization rate\n",
    "\n",
    "2. Extract the numbers from the column `int_rate`. You will have to first use `select` and `show` methods to view what the data looks like. Then you can use the [regex replace technique](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.regexp_replace.html) `regexp_replace` or a python-based user defined function method. **There are both XX.XX% and X.XX% rate types in this column**. Finally, cast `int_rate` to float.\n",
    "\n",
    "4. Filter to only rows with `loan_status` of \"Fully Paid\" or \"Charged Off\". This means that the loan went to \"maturity\" so we can determine if the loan was successful (fully paid) or unsuccessful (charged off).\n",
    "\n",
    "5. Check the potential values for `term`, `home_ownership`, and `grade`. What are the possible categories of each?\n",
    "\n",
    "6. Drop rows with unusual values for on the `home_ownership` variable . There are several categories that are small and have ambiguous meaning. There are three to drop and three to keep.\n",
    "\n",
    "7. Report the number of rows left in your DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Selecting columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_in.select(['int_rate', 'term', 'loan_status', 'home_ownership', 'tot_cur_bal', 'annual_inc', 'grade', 'bc_util'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. String extraction and data type conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:=====================================================>  (18 + 1) / 19]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|int_rate|\n",
      "+--------+\n",
      "|  10.99%|\n",
      "|  10.99%|\n",
      "|  10.99%|\n",
      "|   7.62%|\n",
      "|  12.85%|\n",
      "|   6.62%|\n",
      "|   8.90%|\n",
      "|  16.24%|\n",
      "|   7.62%|\n",
      "|  11.99%|\n",
      "+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# always look at the data before\n",
    "df.select('int_rate').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "df = df.withColumn('int_rate', regexp_replace('int_rate', '%', '').cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 51:=====================================================>  (18 + 1) / 19]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|int_rate|\n",
      "+--------+\n",
      "|   10.99|\n",
      "|   10.99|\n",
      "|   10.99|\n",
      "|    7.62|\n",
      "|   12.85|\n",
      "|    6.62|\n",
      "|     8.9|\n",
      "|   16.24|\n",
      "|    7.62|\n",
      "|   11.99|\n",
      "+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# always look at the data after\n",
    "df.select('int_rate').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Drop loans that have not completed, keeping only `Charged Off` or `Fully Paid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(loan_status='In Grace Period', count=9863),\n",
       " Row(loan_status='Late (31-120 days)', count=15932),\n",
       " Row(loan_status='Issued', count=2047),\n",
       " Row(loan_status='Fully Paid', count=1423664),\n",
       " Row(loan_status='Default', count=422),\n",
       " Row(loan_status='Charged Off', count=347950),\n",
       " Row(loan_status='Current', count=1018833),\n",
       " Row(loan_status='Late (16-30 days)', count=2678)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a count function by distinct value is quite important in pyspark!\n",
    "df.groupby('loan_status').count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df.loan_status.isin(['Charged Off','Fully Paid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:===============>(18 + 1) / 19][Stage 60:===============>(18 + 1) / 19]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+--------------+-----------+----------+-----+-------+\n",
      "|int_rate|      term|loan_status|home_ownership|tot_cur_bal|annual_inc|grade|bc_util|\n",
      "+--------+----------+-----------+--------------+-----------+----------+-----+-------+\n",
      "|   10.99| 36 months| Fully Paid|          RENT|     7137.0|   60000.0|    B|   15.9|\n",
      "|   10.99| 36 months| Fully Paid|      MORTGAGE|     4136.0|   39600.0|    B|   16.1|\n",
      "|   10.99| 36 months| Fully Paid|           OWN|   114834.0|   55000.0|    B|   53.9|\n",
      "|    7.62| 36 months| Fully Paid|      MORTGAGE|   200314.0|   96500.0|    A|   83.5|\n",
      "|   12.85| 36 months| Fully Paid|          RENT|    17672.0|   88000.0|    B|   87.7|\n",
      "|    6.62| 36 months| Fully Paid|      MORTGAGE|   267646.0|  105000.0|    A|   25.0|\n",
      "|     8.9| 36 months| Fully Paid|      MORTGAGE|   272492.0|   63000.0|    A|   79.1|\n",
      "|   16.24| 36 months| Fully Paid|          RENT|     5759.0|   28000.0|    C|   96.0|\n",
      "|    7.62| 36 months| Fully Paid|      MORTGAGE|   799592.0|  325000.0|    A|   67.1|\n",
      "|   11.99| 36 months| Fully Paid|      MORTGAGE|   327264.0|  130000.0|    B|   93.0|\n",
      "+--------+----------+-----------+--------------+-----------+----------+-----+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Check possible values for categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- int_rate: float (nullable = true)\n",
      " |-- term: string (nullable = true)\n",
      " |-- loan_status: string (nullable = true)\n",
      " |-- home_ownership: string (nullable = true)\n",
      " |-- tot_cur_bal: double (nullable = true)\n",
      " |-- annual_inc: double (nullable = true)\n",
      " |-- grade: string (nullable = true)\n",
      " |-- bc_util: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(term=' 60 months', count=446669), Row(term=' 36 months', count=1324945)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('term').count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(home_ownership='MORTGAGE', count=877147),\n",
       " Row(home_ownership='ANY', count=1158),\n",
       " Row(home_ownership='NONE', count=45),\n",
       " Row(home_ownership='RENT', count=695491),\n",
       " Row(home_ownership='OWN', count=197729),\n",
       " Row(home_ownership='OTHER', count=44)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('home_ownership').count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(grade='D', count=261779),\n",
       " Row(grade='B', count=519897),\n",
       " Row(grade='G', count=10004),\n",
       " Row(grade='F', count=34720),\n",
       " Row(grade='C', count=507755),\n",
       " Row(grade='E', count=110052),\n",
       " Row(grade='A', count=327407)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('grade').count().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Drop rows with explicitly missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Report how much data remains after your cleaning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1771614"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run this cell to export 5 rows of your data to a json for grading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import json\n",
    "json.dump({'df' : df.limit(5).toPandas().to_dict('records'),\n",
    "           'home_counts' : [x.asDict() for x in df.groupby('home_ownership').count().collect()]\n",
    "          },\n",
    "          fp = open('data-soln-1.json','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Split data into train, test, and split\n",
    "\n",
    "In this section you will split your data into: train, test and predict datasets. Create three splits of `df_in` (train, test, predict) by using the `randomSplit` method. Read more about the method [here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.randomSplit.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, predict_data = df.randomSplit([0.8, 0.18, 0.02], 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After splitting into three datasets, report the number of rows for each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training records: 1417006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of testing records : 319270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 87:=====================================================>  (23 + 1) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of prediction records : 35338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Number of training records: \" + str(train_data.count()))\n",
    "print(\"Number of testing records : \" + str(test_data.count()))\n",
    "print(\"Number of prediction records : \" + str(predict_data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: Create pipeline and train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, your job is to build a model that classifies the `loan_status`. In this section you will create a machine learning pipeline and then train the model. The next cell imports all the packages you will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, IndexToString, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline, Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following step, convert all the string fields to numeric indices. Look at the DataFrame schema to determine which ones you need to convert. Use the `StringIndexer` estimator. You need to create a transformer for each column you want to modify because the \"model\" has to find out how many distinct options there are for the variable in order to assign consistent integer values.\n",
    "\n",
    "The format of the command will be `StringIndexer(inputCol=\"[INPUT COL NAME]\", outputCol=\"[OUTPUT COL NAME]\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- int_rate: float (nullable = true)\n",
      " |-- term: string (nullable = true)\n",
      " |-- loan_status: string (nullable = true)\n",
      " |-- home_ownership: string (nullable = true)\n",
      " |-- tot_cur_bal: double (nullable = true)\n",
      " |-- annual_inc: double (nullable = true)\n",
      " |-- grade: string (nullable = true)\n",
      " |-- bc_util: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print the schema again\n",
    "train_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringIndexer_loan = StringIndexer(inputCol=\"loan_status\", outputCol=\"loan_success\")\n",
    "stringIndexer_term = StringIndexer(inputCol=\"term\", outputCol=\"term_ix\")\n",
    "stringIndexer_home = StringIndexer(inputCol=\"home_ownership\", outputCol=\"home_ownership_ix\")\n",
    "stringIndexer_grade = StringIndexer(inputCol=\"grade\", outputCol=\"grade_ix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try looking at the values for one of the re-encoded columns using the `labels` method. Does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'StringIndexer' object has no attribute 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30464/3726297414.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstringIndexer_grade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'StringIndexer' object has no attribute 'labels'"
     ]
    }
   ],
   "source": [
    "stringIndexer_grade.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the values of the labels, the estimator needs to be fitted and made a transformer first. You can do so by using the `fit` method. Try that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "indexed = stringIndexer_home.fit(train_data).transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "indexed_t = stringIndexer_grade.fit(indexed).transform(indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "indexed_a = stringIndexer_loan.fit(indexed_t).transform(indexed_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "indexed_b = stringIndexer_term.fit(indexed_a).transform(indexed_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we need to convert our index variables that have more than two levels. Use the function OneHotEncoder [(read more here)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.OneHotEncoder.html). The format is the same as the StringIndexer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_home = OneHotEncoder(inputCol=\"home_ownership_ix\", outputCol=\"home_ownership_vec\")\n",
    "onehot_grade = OneHotEncoder(inputCol=\"grade_ix\", outputCol=\"grade_vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = onehot_home.fit(indexed_b).transform(indexed_b)\n",
    "f2 = onehot_grade.fit(f1).transform(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following step, create a feature vector by combining all string features together using the `vectorAssembler` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler_features = VectorAssembler(\n",
    "    inputCols=[\"term_ix\", \"home_ownership_vec\", \"grade_vec\"], \n",
    "    outputCol= \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the resulting transformer look like? There is no data here because the transformer is just the blueprint for the change in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 167:====================================================>  (18 + 1) / 19]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+--------------+-----------+----------+-----+-------+-----------------+--------+------------+-------+------------------+-------------+--------------------+\n",
      "|int_rate|      term|loan_status|home_ownership|tot_cur_bal|annual_inc|grade|bc_util|home_ownership_ix|grade_ix|loan_success|term_ix|home_ownership_vec|    grade_vec|            features|\n",
      "+--------+----------+-----------+--------------+-----------+----------+-----+-------+-----------------+--------+------------+-------+------------------+-------------+--------------------+\n",
      "|    5.31| 36 months|Charged Off|      MORTGAGE|    11518.0|   42000.0|    A|   23.2|              0.0|     2.0|         1.0|    0.0|     (5,[0],[1.0])|(6,[2],[1.0])|(12,[1,8],[1.0,1.0])|\n",
      "|    5.31| 36 months|Charged Off|      MORTGAGE|    13153.0|   65400.0|    A|   70.3|              0.0|     2.0|         1.0|    0.0|     (5,[0],[1.0])|(6,[2],[1.0])|(12,[1,8],[1.0,1.0])|\n",
      "|    5.31| 36 months|Charged Off|      MORTGAGE|    15169.0|   85000.0|    A|   22.2|              0.0|     2.0|         1.0|    0.0|     (5,[0],[1.0])|(6,[2],[1.0])|(12,[1,8],[1.0,1.0])|\n",
      "|    5.31| 36 months|Charged Off|      MORTGAGE|    16313.0|  130000.0|    A|   21.4|              0.0|     2.0|         1.0|    0.0|     (5,[0],[1.0])|(6,[2],[1.0])|(12,[1,8],[1.0,1.0])|\n",
      "|    5.31| 36 months|Charged Off|      MORTGAGE|    16848.0|   45000.0|    A|   13.3|              0.0|     2.0|         1.0|    0.0|     (5,[0],[1.0])|(6,[2],[1.0])|(12,[1,8],[1.0,1.0])|\n",
      "|    5.31| 36 months|Charged Off|      MORTGAGE|    24527.0|   78000.0|    A|   50.5|              0.0|     2.0|         1.0|    0.0|     (5,[0],[1.0])|(6,[2],[1.0])|(12,[1,8],[1.0,1.0])|\n",
      "|    5.31| 36 months|Charged Off|      MORTGAGE|    24991.0|   18000.0|    A|   19.4|              0.0|     2.0|         1.0|    0.0|     (5,[0],[1.0])|(6,[2],[1.0])|(12,[1,8],[1.0,1.0])|\n",
      "|    5.31| 36 months|Charged Off|      MORTGAGE|    29641.0|  161000.0|    A|   29.8|              0.0|     2.0|         1.0|    0.0|     (5,[0],[1.0])|(6,[2],[1.0])|(12,[1,8],[1.0,1.0])|\n",
      "|    5.31| 36 months|Charged Off|      MORTGAGE|    37312.0|   90000.0|    A|   21.2|              0.0|     2.0|         1.0|    0.0|     (5,[0],[1.0])|(6,[2],[1.0])|(12,[1,8],[1.0,1.0])|\n",
      "|    5.31| 36 months|Charged Off|      MORTGAGE|    61380.0|   40000.0|    A|   30.2|              0.0|     2.0|         1.0|    0.0|     (5,[0],[1.0])|(6,[2],[1.0])|(12,[1,8],[1.0,1.0])|\n",
      "+--------+----------+-----------+--------------+-----------+----------+-----+-------+-----------------+--------+------------+-------+------------------+-------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "feature_vectorized = vectorAssembler_features.transform(f2)\n",
    "feature_vectorized.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define the estimator you want to use for classification. You will build a Random Forest using the `RandomForestClassifier` [estimator](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.RandomForestClassifier.html). This function is also where you would adjust hyperparameters for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"loan_success\", featuresCol=\"features\", numTrees=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, indexed labels back to original labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelConverter = IndexToString(inputCol=\"prediction\", \n",
    "                               outputCol=\"predictedLoanStatus\", \n",
    "                               labels= ['Fully Paid','Charged Off'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the pipeline now. A pipeline consists of transformers and an estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_rf = Pipeline(stages=[stringIndexer_loan, \n",
    "                               stringIndexer_term, \n",
    "                               stringIndexer_home, \n",
    "                               stringIndexer_grade, \n",
    "                               onehot_home,\n",
    "                               onehot_grade,\n",
    "                               vectorAssembler_features, \n",
    "                               rf, labelConverter])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can train your Random Forest model by using the previously defined **pipeline** and **train data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- int_rate: float (nullable = true)\n",
      " |-- term: string (nullable = true)\n",
      " |-- loan_status: string (nullable = true)\n",
      " |-- home_ownership: string (nullable = true)\n",
      " |-- tot_cur_bal: double (nullable = true)\n",
      " |-- annual_inc: double (nullable = true)\n",
      " |-- grade: string (nullable = true)\n",
      " |-- bc_util: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 164:====================================================>  (18 + 1) / 19]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+--------------+-----------+----------+-----+-------+\n",
      "|int_rate|      term|loan_status|home_ownership|tot_cur_bal|annual_inc|grade|bc_util|\n",
      "+--------+----------+-----------+--------------+-----------+----------+-----+-------+\n",
      "|    5.31| 36 months|Charged Off|      MORTGAGE|    11518.0|   42000.0|    A|   23.2|\n",
      "|    5.31| 36 months|Charged Off|      MORTGAGE|    13153.0|   65400.0|    A|   70.3|\n",
      "|    5.31| 36 months|Charged Off|      MORTGAGE|    15169.0|   85000.0|    A|   22.2|\n",
      "|    5.31| 36 months|Charged Off|      MORTGAGE|    16313.0|  130000.0|    A|   21.4|\n",
      "|    5.31| 36 months|Charged Off|      MORTGAGE|    16848.0|   45000.0|    A|   13.3|\n",
      "|    5.31| 36 months|Charged Off|      MORTGAGE|    24527.0|   78000.0|    A|   50.5|\n",
      "|    5.31| 36 months|Charged Off|      MORTGAGE|    24991.0|   18000.0|    A|   19.4|\n",
      "|    5.31| 36 months|Charged Off|      MORTGAGE|    29641.0|  161000.0|    A|   29.8|\n",
      "|    5.31| 36 months|Charged Off|      MORTGAGE|    37312.0|   90000.0|    A|   21.2|\n",
      "|    5.31| 36 months|Charged Off|      MORTGAGE|    61380.0|   40000.0|    A|   30.2|\n",
      "+--------+----------+-----------+--------------+-----------+----------+-----+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model_rf = pipeline_rf.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does `model_rf` look like? What is it, an estimator or a transformer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineModel_eb205cf38a7b\n"
     ]
    }
   ],
   "source": [
    "print(model_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens when you send the train data through the transform method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[int_rate: float, term: string, loan_status: string, home_ownership: string, tot_cur_bal: double, annual_inc: double, grade: string, bc_util: double, loan_success: double, term_ix: double, home_ownership_ix: double, grade_ix: double, home_ownership_vec: vector, grade_vec: vector, features: vector, rawPrediction: vector, probability: vector, prediction: double, predictedLoanStatus: string]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_rf.fit(train_data).transform(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Model Test Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will evaluate the **model accuracy**. This requires using the evaluator `MulticlassClassificationEvaluator`, which you can read about in the doc [here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.MulticlassClassificationEvaluator.html). To evaluate the model, use **test data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's transform the test data using our model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_rf.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's run the `MulticlassClassificationEvaluator` by passing in the label column (acutal result), prediction column (from our model), and the metric we want to calculate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "evaluatorRF = MulticlassClassificationEvaluator(labelCol=\"loan_success\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluatorRF.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "evaluatorRF = MulticlassClassificationEvaluator(labelCol=\"loan_success\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluatorRF.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.803931\n",
      "Test Error = 0.196069\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy = %g\" % accuracy)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was the test error similar to the train error? Run the same accuracy metrics on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_new = model_rf.transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "evaluatorRF = MulticlassClassificationEvaluator(labelCol=\"loan_success\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluatorRF.evaluate(predictions_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.803498\n",
      "Train Error = 0.196502\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy = %g\" % accuracy)\n",
    "print(\"Train Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will build the confusion matrix from the model. We will use sci-kit learn to build the confusion matrix. We first have to extract the predicted label and the true label columns. These are the numeric binary form of the data. Finally, send them through the `confusion_matrix` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "y_pred=predictions.select(\"prediction\").collect()\n",
    "y_orig=predictions.select(\"loan_success\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[256671      0]\n",
      " [ 62599      0]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_orig, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 ROC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will evaluate the **model area under the curve**. This requires using the evaluator `BinaryClassificationEvaluator`, which you can read about in the doc [here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.BinaryClassificationEvaluator.html). To evaluate the model, use **test data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluatorRF = BinaryClassificationEvaluator(labelCol=\"loan_success\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "roc_result = evaluatorRF.evaluate(predictions)\n",
    "roc_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run this cell to export model metrics to a json for grading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "json.dump({'roc' : roc_result,\n",
    "           'cm' : str(cm),\n",
    "           'acc' : accuracy\n",
    "          },\n",
    "          fp = open('data-soln-2.json','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MAKE SURE YOU STOP YOUR EMR CLUSTER!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
