{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis and Data Prep in PySpark\n",
    "\n",
    "In this workbook, you will read in the `trip` and `fare` files. You are welcome to use DataFrame and/or SparkSQL API as you desire as long as it produces the expected results.\n",
    "\n",
    "It is recommended to make small versions of the datasets for testing purposes so that you can trial and error faster. Run a command similar to `df_small = df.limit(10000)`\n",
    "\n",
    "Make sure you use your [PySparkSQL Cheat Sheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf) to help you with the commands to complete the assignment.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "1. Read in both datasets and conduct initial exploration. \n",
    "    - Determine the shape of the data, column names, data types\n",
    "    - Check for the number of missing values on the critical fields of the datasets - pickup_datetime, dropoff_datetime, passenger_count, trip_distance, fare_amount, medallion, hack_license, etc. There is a `count` command to use with columns in a dataframe and several ways to check for missing values. Try Googling some options!\n",
    "    - Check the counts for values of passenger_count and remove any outlier values (how many people fit into a taxi?). Justify your analytical decision. You can use the `.isin()` method to create boolean values from a column ([see here](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.Column.isin.html)).\n",
    "\n",
    "2. Join both datasets such that you get a merged dataset with 21 unique fields. \n",
    "    - Which columns do you use to join the data? You need to determine how to match the schemas.\n",
    "    - What type of join should you use? Try at least two types of joins and report how much data is matched and is lost based on the join type. Justify which join you selected.\n",
    "\n",
    "3. Once you create the merged dataset, you need to convert fields to the following types, since all fields were read in as string:\n",
    "    * pickup_datetime and dropoff_datetime must be TIMESTAMP\n",
    "    * passenger_count and rate_code must be INT\n",
    "    * all other numeric fields must be FLOAT\n",
    "    * the remaining fields stay as STRING\n",
    "\n",
    "4. Create new variables to your dataset that will be important for understanding the data.\n",
    "    * Dummy variable for if pickup_datetime appears on a weekend or not\n",
    "    * Dummy variable for if pickup_datetime appears during weekday rush hour (6:00-9:59am, 2:00-5:59pm) or not\n",
    "    * Dummy variable for if the tip_amount is greater than 10% of the fare_amount or not\n",
    "\n",
    "5. Save this merged, converted, and transformed dataset to your own S3 bucket in parquet format.\n",
    "\n",
    "6. Report the average, median, Q25, and Q75 fare amount in a table conditioned on the dummy variables you created in step 4. Each table should look like the example below, though your numbers might not match exactly.\n",
    "\n",
    "\n",
    " |  weekdayrush_dummy  |mean_fare  |Q25|  median|   Q75|  num_rides|\n",
    "|----------------------|-----------|---|--------|------|-----------|\n",
    "|0               |True     |12.462  |6.5     |9.0  |14.0   |44848941|\n",
    "|1              |False     |12.312  |6.5     |9.5  |14.0  |128336134|\n",
    "       \n",
    "You are welcome to add as many cells as you need. Clearly indicate in your notebook each step so graders can confirm you have accomplished each task. **You must include comments in your code.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REQUIRED:** Start off by copying the taxi data from the public S3 bucket to your personal S3 bucket!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-19 18:59:54,990 INFO tools.DistCp: Input Options: DistCpOptions{atomicCommit=false, syncFolder=false, deleteMissing=false, ignoreFailures=false, overwrite=false, append=false, useDiff=false, useRdiff=false, fromSnapshot=null, toSnapshot=null, skipCRC=false, blocking=true, numListstatusThreads=0, maxMaps=20, mapBandwidth=0.0, copyStrategy='uniformsize', preserveStatus=[BLOCKSIZE], atomicWorkPath=null, logPath=null, sourceFileListing=null, sourcePaths=[s3://bigdatateaching/nyctaxi-2013/parquet], targetPath=s3://anly502-fall-2022-yl1353/taxi, filtersFile='null', blocksPerChunk=0, copyBufferSize=8192, verboseLog=false, directWrite=false}, sourcePaths=[s3://bigdatateaching/nyctaxi-2013/parquet], targetPathExists=true, preserveRawXattrsfalse\n",
      "2022-10-19 18:59:55,227 INFO client.RMProxy: Connecting to ResourceManager at ip-172-31-39-182.ec2.internal/172.31.39.182:8032\n",
      "2022-10-19 18:59:55,436 INFO client.AHSProxy: Connecting to Application History server at ip-172-31-39-182.ec2.internal/172.31.39.182:10200\n",
      "2022-10-19 18:59:57,253 INFO tools.SimpleCopyListing: Paths (files+dirs) cnt = 371; dirCnt = 3\n",
      "2022-10-19 18:59:57,253 INFO tools.SimpleCopyListing: Build file listing completed.\n",
      "2022-10-19 18:59:57,255 INFO Configuration.deprecation: io.sort.mb is deprecated. Instead, use mapreduce.task.io.sort.mb\n",
      "2022-10-19 18:59:57,255 INFO Configuration.deprecation: io.sort.factor is deprecated. Instead, use mapreduce.task.io.sort.factor\n",
      "2022-10-19 18:59:57,572 INFO tools.DistCp: Number of paths in the copy list: 371\n",
      "2022-10-19 18:59:57,749 INFO tools.DistCp: Number of paths in the copy list: 371\n",
      "2022-10-19 18:59:57,766 INFO client.RMProxy: Connecting to ResourceManager at ip-172-31-39-182.ec2.internal/172.31.39.182:8032\n",
      "2022-10-19 18:59:57,766 INFO client.AHSProxy: Connecting to Application History server at ip-172-31-39-182.ec2.internal/172.31.39.182:10200\n",
      "2022-10-19 18:59:57,841 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1666202505042_0001\n",
      "2022-10-19 18:59:58,300 INFO mapreduce.JobSubmitter: number of splits:21\n",
      "2022-10-19 18:59:58,416 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1666202505042_0001\n",
      "2022-10-19 18:59:58,418 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2022-10-19 18:59:58,572 INFO conf.Configuration: resource-types.xml not found\n",
      "2022-10-19 18:59:58,573 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2022-10-19 18:59:58,980 INFO impl.YarnClientImpl: Submitted application application_1666202505042_0001\n",
      "2022-10-19 18:59:59,037 INFO mapreduce.Job: The url to track the job: http://ip-172-31-39-182.ec2.internal:20888/proxy/application_1666202505042_0001/\n",
      "2022-10-19 18:59:59,038 INFO tools.DistCp: DistCp job-id: job_1666202505042_0001\n",
      "2022-10-19 18:59:59,039 INFO mapreduce.Job: Running job: job_1666202505042_0001\n",
      "2022-10-19 19:00:05,126 INFO mapreduce.Job: Job job_1666202505042_0001 running in uber mode : false\n",
      "2022-10-19 19:00:05,127 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2022-10-19 19:00:16,221 INFO mapreduce.Job:  map 14% reduce 0%\n",
      "2022-10-19 19:00:17,225 INFO mapreduce.Job:  map 29% reduce 0%\n",
      "2022-10-19 19:00:18,229 INFO mapreduce.Job:  map 43% reduce 0%\n",
      "2022-10-19 19:00:21,243 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2022-10-19 19:00:22,248 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2022-10-19 19:00:25,264 INFO mapreduce.Job: Job job_1666202505042_0001 completed successfully\n",
      "2022-10-19 19:00:25,362 INFO mapreduce.Job: Counters: 42\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=5074332\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=120728\n",
      "\t\tHDFS: Number of bytes written=44410\n",
      "\t\tHDFS: Number of read operations=168\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=42\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tS3: Number of bytes read=0\n",
      "\t\tS3: Number of bytes written=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=21\n",
      "\t\tOther local map tasks=21\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=23606688\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=245903\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=245903\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=755414016\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=371\n",
      "\t\tMap output records=368\n",
      "\t\tInput split bytes=2877\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=6902\n",
      "\t\tCPU time spent (ms)=144770\n",
      "\t\tPhysical memory (bytes) snapshot=9082855424\n",
      "\t\tVirtual memory (bytes) snapshot=93611565056\n",
      "\t\tTotal committed heap usage (bytes)=8530690048\n",
      "\t\tPeak Map Physical memory (bytes)=546672640\n",
      "\t\tPeak Map Virtual memory (bytes)=4486873088\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=117851\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=44410\n",
      "\tDistCp Counters\n",
      "\t\tBandwidth in Btyes=0\n",
      "\t\tBytes Skipped=6692038168\n",
      "\t\tDIR_COPY=3\n",
      "\t\tFiles Skipped=368\n"
     ]
    }
   ],
   "source": [
    "!hadoop distcp s3://bigdatateaching/nyctaxi-2013/parquet/ s3://anly502-fall-2022-yl1353/taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/10/19 19:00:36 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "22/10/19 19:00:45 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext()\n",
    "spark = SparkSession.builder.appName(\"taxi-assignment\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Read in data and explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "trip = spark.read.parquet(\"s3://anly502-fall-2022-yl1353/taxi/trip/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('medallion', 'string'),\n",
       " ('hack_license', 'string'),\n",
       " ('vendor_id', 'string'),\n",
       " ('rate_code', 'string'),\n",
       " ('store_and_fwd_flag', 'string'),\n",
       " ('pickup_datetime', 'string'),\n",
       " ('dropoff_datetime', 'string'),\n",
       " ('passenger_count', 'string'),\n",
       " ('trip_time_in_secs', 'string'),\n",
       " ('trip_distance', 'string'),\n",
       " ('pickup_longitude', 'string'),\n",
       " ('pickup_latitude', 'string'),\n",
       " ('dropoff_longitude', 'string'),\n",
       " ('dropoff_latitude', 'string')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:========================================================>(62 + 1) / 63]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+---------+---------+------------------+---------------+----------------+---------------+-----------------+-------------+----------------+---------------+-----------------+----------------+\n",
      "|medallion|hack_license|vendor_id|rate_code|store_and_fwd_flag|pickup_datetime|dropoff_datetime|passenger_count|trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|\n",
      "+---------+------------+---------+---------+------------------+---------------+----------------+---------------+-----------------+-------------+----------------+---------------+-----------------+----------------+\n",
      "|        0|           0|        0|        0|          86519129|              0|               0|              0|                0|            0|               0|              0|             3438|            3438|\n",
      "+---------+------------+---------+---------+------------------+---------------+----------------+---------------+-----------------+-------------+----------------+---------------+-----------------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,sum\n",
    "trip.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in trip.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+\n",
      "|passenger_count|    count|\n",
      "+---------------+---------+\n",
      "|              7|       35|\n",
      "|              9|       26|\n",
      "|              3|  7315829|\n",
      "|              0|     5035|\n",
      "|            208|       13|\n",
      "|            129|        1|\n",
      "|              6|  6764789|\n",
      "|              1|121959711|\n",
      "|              8|       25|\n",
      "|              5| 10034696|\n",
      "|            255|        2|\n",
      "|              4|  3582103|\n",
      "|              2| 23517494|\n",
      "+---------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trip_c = trip.groupBy(\"passenger_count\").count()\n",
    "trip_c.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove outliers: 208,129,255\n",
    "trip_p = trip_c[trip_c.passenger_count.isin([0,1,2,3,4,5,6])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:=================================================>      (56 + 7) / 63]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+\n",
      "|passenger_count|    count|\n",
      "+---------------+---------+\n",
      "|              3|  7315829|\n",
      "|              0|     5035|\n",
      "|              6|  6764789|\n",
      "|              1|121959711|\n",
      "|              5| 10034696|\n",
      "|              4|  3582103|\n",
      "|              2| 23517494|\n",
      "+---------------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "trip_p.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fare = spark.read.parquet(\"s3://anly502-fall-2022-yl1353/taxi/fare/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('medallion', 'string'),\n",
       " ('hack_license', 'string'),\n",
       " ('vendor_id', 'string'),\n",
       " ('pickup_datetime', 'string'),\n",
       " ('payment_type', 'string'),\n",
       " ('fare_amount', 'string'),\n",
       " ('surcharge', 'string'),\n",
       " ('mta_tax', 'string'),\n",
       " ('tip_amount', 'string'),\n",
       " ('tolls_amount', 'string'),\n",
       " ('total_amount', 'string')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fare.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:=======================================================>(65 + 1) / 66]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+---------+---------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "|medallion|hack_license|vendor_id|pickup_datetime|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|total_amount|\n",
      "+---------+------------+---------+---------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "|        0|           0|        0|              0|           0|          0|        0|      0|         0|           0|           0|\n",
      "+---------+------------+---------+---------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fare.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in fare.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Join the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df_inner = trip.join(fare,['medallion','hack_license','vendor_id','pickup_datetime'],'inner').cache()\n",
    "#merge_df = pd.merge(trip, fare, on='medallion')#,'hack_license', 'vendor_id','pickup_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df_outer = trip.join(fare,['medallion','hack_license','vendor_id','pickup_datetime'],\"outer\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_count_inner: 173185091\n",
      "col_count_inner: 21\n"
     ]
    }
   ],
   "source": [
    "print('row_count_inner:', merge_df_inner.count())\n",
    "print('col_count_inner:', len(merge_df_inner.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lost approx. 16k from original size of dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_count_outer: 173185091\n",
      "col_count_outer: 21\n"
     ]
    }
   ],
   "source": [
    "print('row_count_outer:', merge_df_outer.count())\n",
    "print('col_count_outer:', len(merge_df_outer.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lost approx. 16k from original size of dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think the inner join makes more sense here, but I am a little confused that the final dataset generated from inner join and outer join have the same numbers of rows and columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Change data types\n",
    "This step can also be integrated into Step 2 merging if using PySparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "merge_df_inner = merge_df_inner.withColumn(\"pickup_datetime\",to_timestamp(\"pickup_datetime\")) \n",
    "df = merge_df_inner.withColumn(\"dropoff_datetime\",to_timestamp(\"dropoff_datetime\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "df = df.withColumn(\"passenger_count\", df[\"passenger_count\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"rate_code\", df[\"rate_code\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- hack_license: string (nullable = true)\n",
      " |-- vendor_id: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- rate_code: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_time_in_secs: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- pickup_longitude: string (nullable = true)\n",
      " |-- pickup_latitude: string (nullable = true)\n",
      " |-- dropoff_longitude: string (nullable = true)\n",
      " |-- dropoff_latitude: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- surcharge: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_col = ['trip_time_in_secs', 'trip_distance', 'pickup_longitude', 'pickup_latitude',\n",
    "             'dropoff_longitude', 'dropoff_latitude', 'fare_amount', 'surcharge', 'mta_tax',\n",
    "             'tip_amount',  'tolls_amount', 'total_amount']\n",
    "for cols in num_col:\n",
    "    df = df.withColumn(cols, col(cols).cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- hack_license: string (nullable = true)\n",
      " |-- vendor_id: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- rate_code: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_time_in_secs: float (nullable = true)\n",
      " |-- trip_distance: float (nullable = true)\n",
      " |-- pickup_longitude: float (nullable = true)\n",
      " |-- pickup_latitude: float (nullable = true)\n",
      " |-- dropoff_longitude: float (nullable = true)\n",
      " |-- dropoff_latitude: float (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: float (nullable = true)\n",
      " |-- surcharge: float (nullable = true)\n",
      " |-- mta_tax: float (nullable = true)\n",
      " |-- tip_amount: float (nullable = true)\n",
      " |-- tolls_amount: float (nullable = true)\n",
      " |-- total_amount: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create dummy variables\n",
    "\n",
    "- Dummy variable for if pickup_datetime appears on a weekend or not\n",
    "- Dummy variable for if pickup_datetime appears during weekday rush hour (6:00-9:59am, 2:00-5:59pm) or not\n",
    "- Dummy variable for if the tip_amount is greater than 10% of the fare_amount or not\n",
    "\n",
    "Reminder that using a small dataframe for testing will help you develop your analytics faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dayofweek\n",
    "df = df.withColumn(\"pickup_weekend_dummy\", dayofweek(\"pickup_datetime\").isin([1,7]).cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('time', date_format('pickup_datetime', 'HH:mm:ss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"weekdayrush_dummy\",\n",
    "                   when((col(\"pickup_weekend_dummy\") == 0) & \n",
    "                        ((col(\"time\").between('6:00:00','9:59:00'))|\n",
    "                         (col(\"time\").between('14:00:00','17:59:00'))),1)\n",
    "                   .otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('tip_amount_dummy', when(col('tip_amount') > col('fare_amount')*0.1, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(medallion='00005007A9F30E289E760362F69E4EAD', hack_license='02015F5B7D18846209A4DEDF896F6D9C', vendor_id='CMT', pickup_datetime=datetime.datetime(2013, 5, 20, 2, 15, 5), rate_code=1, store_and_fwd_flag='N', dropoff_datetime=datetime.datetime(2013, 5, 20, 2, 28, 51), passenger_count=2, trip_time_in_secs=827.0, trip_distance=5.300000190734863, pickup_longitude=-74.0083999633789, pickup_latitude=40.740516662597656, dropoff_longitude=-73.96519470214844, dropoff_latitude=40.801292419433594, payment_type='CRD', fare_amount=17.5, surcharge=0.5, mta_tax=0.5, tip_amount=3.700000047683716, tolls_amount=0.0, total_amount=22.200000762939453, pickup_weekend_dummy=0, time='02:15:05', weekdayrush_dummy=0, tip_amount_dummy=1),\n",
       " Row(medallion='00005007A9F30E289E760362F69E4EAD', hack_license='0649DA10C83DE7C6A1CD6BDC97DAB068', vendor_id='CMT', pickup_datetime=datetime.datetime(2013, 5, 14, 20, 28, 38), rate_code=1, store_and_fwd_flag='N', dropoff_datetime=datetime.datetime(2013, 5, 14, 20, 37, 35), passenger_count=1, trip_time_in_secs=536.0, trip_distance=1.7000000476837158, pickup_longitude=-73.97930145263672, pickup_latitude=40.73579025268555, dropoff_longitude=-74.0035400390625, dropoff_latitude=40.74030685424805, payment_type='CRD', fare_amount=8.5, surcharge=0.5, mta_tax=0.5, tip_amount=1.899999976158142, tolls_amount=0.0, total_amount=11.399999618530273, pickup_weekend_dummy=0, time='20:28:38', weekdayrush_dummy=0, tip_amount_dummy=1),\n",
       " Row(medallion='00005007A9F30E289E760362F69E4EAD', hack_license='0FD461760B482C0B229B3E98A2914819', vendor_id='CMT', pickup_datetime=datetime.datetime(2013, 11, 18, 1, 41, 18), rate_code=1, store_and_fwd_flag='N', dropoff_datetime=datetime.datetime(2013, 11, 18, 1, 53, 22), passenger_count=1, trip_time_in_secs=723.0, trip_distance=3.200000047683716, pickup_longitude=-73.98717498779297, pickup_latitude=40.72225570678711, dropoff_longitude=-73.94966888427734, dropoff_latitude=40.715641021728516, payment_type='CSH', fare_amount=12.0, surcharge=0.5, mta_tax=0.5, tip_amount=0.0, tolls_amount=0.0, total_amount=13.0, pickup_weekend_dummy=0, time='01:41:18', weekdayrush_dummy=0, tip_amount_dummy=0)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Save data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.parquet(\"s3://anly502-fall-2022-yl1353/taxi/a6-part2/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#table for weekend\n",
    "df_weekend = (df.groupBy((when(col(\"pickup_weekend_dummy\") == 1, 'True').\n",
    "                          otherwise('False')).alias('pickup_weekend_dummy')) #groupby by weekend or not\n",
    "              \n",
    "              #Report the average, median, Q25, and Q75 fare amount\n",
    "              .agg(mean('fare_amount').alias('mean_fare'),\n",
    "                   expr('percentile(fare_amount, array(0.25))')[0].alias('Q25'),\n",
    "                   expr('percentile(fare_amount, array(0.5))')[0].alias('median'),\n",
    "                   expr('percentile(fare_amount, array(0.75))')[0].alias('Q75'), \n",
    "                   count(lit(1)).alias('num_rides'))\n",
    "              .toPandas()) #convert to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_weekend_dummy</th>\n",
       "      <th>mean_fare</th>\n",
       "      <th>Q25</th>\n",
       "      <th>median</th>\n",
       "      <th>Q75</th>\n",
       "      <th>num_rides</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>12.267475</td>\n",
       "      <td>6.5</td>\n",
       "      <td>9.5</td>\n",
       "      <td>14.0</td>\n",
       "      <td>49169382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>12.383558</td>\n",
       "      <td>6.5</td>\n",
       "      <td>9.5</td>\n",
       "      <td>14.0</td>\n",
       "      <td>124015709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pickup_weekend_dummy  mean_fare  Q25  median   Q75  num_rides\n",
       "0                 True  12.267475  6.5     9.5  14.0   49169382\n",
       "1                False  12.383558  6.5     9.5  14.0  124015709"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#table for rushhour\n",
    "df_rushhour = (df.groupBy((when(col(\"weekdayrush_dummy\") == 1, 'True').\n",
    "                           otherwise('False')).alias('weekdayrush_dummy')) #groupby by weekday rushhour or not\n",
    "               \n",
    "               #Report the average, median, Q25, and Q75 fare amount\n",
    "               .agg(mean('fare_amount').alias('mean_fare'),\n",
    "                    expr('percentile(fare_amount, array(0.25))')[0].alias('Q25'),\n",
    "                    expr('percentile(fare_amount, array(0.5))')[0].alias('median'),\n",
    "                    expr('percentile(fare_amount, array(0.75))')[0].alias('Q75'),\n",
    "                    count(lit(1)).alias('num_rides'))\n",
    "               .toPandas()) #convert to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weekdayrush_dummy</th>\n",
       "      <th>mean_fare</th>\n",
       "      <th>Q25</th>\n",
       "      <th>median</th>\n",
       "      <th>Q75</th>\n",
       "      <th>num_rides</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>12.995909</td>\n",
       "      <td>6.5</td>\n",
       "      <td>9.5</td>\n",
       "      <td>14.5</td>\n",
       "      <td>22901264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>12.252265</td>\n",
       "      <td>6.5</td>\n",
       "      <td>9.5</td>\n",
       "      <td>14.0</td>\n",
       "      <td>150283827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  weekdayrush_dummy  mean_fare  Q25  median   Q75  num_rides\n",
       "0              True  12.995909  6.5     9.5  14.5   22901264\n",
       "1             False  12.252265  6.5     9.5  14.0  150283827"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rushhour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_tip = (df.groupBy((when(col(\"tip_amount_dummy\") == 1, 'True').\n",
    "                      otherwise('False')).alias('tip_amount_dummy')) #groupby by tip_amount is greater than 10% of the fare_amount or not\n",
    "          \n",
    "          #Report the average, median, Q25, and Q75 fare amount\n",
    "          .agg(mean('fare_amount').alias('mean_fare'),\n",
    "               expr('percentile(fare_amount, array(0.25))')[0].alias('Q25'),\n",
    "               expr('percentile(fare_amount, array(0.5))')[0].alias('median'),\n",
    "               expr('percentile(fare_amount, array(0.75))')[0].alias('Q75'),\n",
    "               count(lit(1)).alias('num_rides'))\n",
    "          .toPandas()) #convert to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tip_amount_dummy</th>\n",
       "      <th>mean_fare</th>\n",
       "      <th>Q25</th>\n",
       "      <th>median</th>\n",
       "      <th>Q75</th>\n",
       "      <th>num_rides</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>12.766442</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>14.5</td>\n",
       "      <td>83788961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>11.960842</td>\n",
       "      <td>6.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>89396130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tip_amount_dummy  mean_fare  Q25  median   Q75  num_rides\n",
       "0             True  12.766442  7.0     9.5  14.5   83788961\n",
       "1            False  11.960842  6.5     9.0  13.5   89396130"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Save your analytics results to a json object - then add, commit, and push your notebook and json to GitHub!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dump({'weekend' : df_weekend.to_dict('records'),\n",
    "           'tip' : df_tip.to_dict('records'),\n",
    "           'rushhour' : df_rushhour.to_dict('records')\n",
    "          }, \n",
    "          fp = open('taxi-soln.json','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MAKE SURE YOU STOP YOUR EMR CLUSTER!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
