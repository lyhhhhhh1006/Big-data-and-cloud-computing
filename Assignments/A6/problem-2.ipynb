{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 - Working with SparkSQL\n",
    "\n",
    "This is an interactive PySpark session. Remember that when you open this notebook the `SparkContext` and `SparkSession` are already created, and they are in the `sc` and `spark` variables, respectively. You can run the following two cells to make sure that the Kernel is active.\n",
    "\n",
    "**Do not insert any additional cells than the ones that are provided.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/10/18 03:51:21 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "22/10/18 03:51:28 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "sc = SparkContext()\n",
    "spark = SparkSession.builder.appName(\"problem2\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quazyilx again!\n",
    "\n",
    "Yes, you remember it. As a reminder, here is the description of the files.\n",
    "\n",
    "The quazyilx has been malfunctioning, and occasionally generates output with a `-1` for all four measurements, like this:\n",
    "\n",
    "    2015-12-10T08:40:10Z fnard:-1 fnok:-1 cark:-1 gnuck:-1\n",
    "\n",
    "There are four different versions of the _quazyilx_ file, each of a different size. As you can see in the output below the file sizes are 50MB (1,000,000 rows), 4.8GB (100,000,000 rows), 18GB (369,865,098 rows) and 36.7GB (752,981,134 rows). The only difference is the length of the number of records, the file structure is the same.\n",
    "\n",
    "```\n",
    "[hadoop@ip-172-31-1-240 ~]$ hadoop fs -ls s3://bigdatateaching/quazyilx/\n",
    "Found 4 items\n",
    "-rw-rw-rw-   1 hadoop hadoop    52443735 2018-01-25 15:37 s3://bigdatateaching/quazyilx/quazyilx0.txt\n",
    "-rw-rw-rw-   1 hadoop hadoop  5244417004 2018-01-25 15:37 s3://bigdatateaching/quazyilx/quazyilx1.txt\n",
    "-rw-rw-rw-   1 hadoop hadoop 19397230888 2018-01-25 15:38 s3://bigdatateaching/quazyilx/quazyilx2.txt\n",
    "-rw-rw-rw-   1 hadoop hadoop 39489364082 2018-01-25 15:41 s3://bigdatateaching/quazyilx/quazyilx3.txt\n",
    "```\n",
    "\n",
    "You will use Spark to create a Spark RDD and then run some analysis on the files using custom functions and Spark RDDs.\n",
    "\n",
    "Start off by copying the quazyilx1.txt file from the central bucket to your personal bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy: s3://bigdatateaching/quazyilx/quazyilx1.txt to s3://anly502-fall-2022-yl1353/quazyilx1.txt\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://bigdatateaching/quazyilx/quazyilx1.txt s3://anly502-fall-2022-yl1353/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, create an RDD called `quazyilx` that reads the `quazyilx1.txt` file from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "quazyilx = sc.textFile(\"s3://anly502-fall-2022-yl1353/quazyilx1.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, Evaluate `quazyilx.take(100)` to make sure that everything is working corectly. This should take a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2000-01-01 00:00:03 fnard:7 fnok:8 cark:19 gnuck:25',\n",
       " '2000-01-01 00:00:08 fnard:14 fnok:19 cark:16 gnuck:37',\n",
       " '2000-01-01 00:00:17 fnard:12 fnok:11 cark:12 gnuck:8',\n",
       " '2000-01-01 00:00:22 fnard:18 fnok:16 cark:3 gnuck:8',\n",
       " '2000-01-01 00:00:32 fnard:7 fnok:16 cark:7 gnuck:37',\n",
       " '2000-01-01 00:00:40 fnard:6 fnok:14 cark:3 gnuck:30',\n",
       " '2000-01-01 00:00:47 fnard:11 fnok:10 cark:17 gnuck:7',\n",
       " '2000-01-01 00:00:55 fnard:9 fnok:14 cark:13 gnuck:30',\n",
       " '2000-01-01 00:00:56 fnard:10 fnok:1 cark:7 gnuck:6',\n",
       " '2000-01-01 00:00:59 fnard:11 fnok:11 cark:12 gnuck:18',\n",
       " '2000-01-01 00:01:03 fnard:9 fnok:13 cark:14 gnuck:49',\n",
       " '2000-01-01 00:01:06 fnard:12 fnok:10 cark:19 gnuck:30',\n",
       " '2000-01-01 00:01:16 fnard:0 fnok:12 cark:19 gnuck:26',\n",
       " '2000-01-01 00:01:26 fnard:10 fnok:11 cark:10 gnuck:49',\n",
       " '2000-01-01 00:01:30 fnard:9 fnok:5 cark:16 gnuck:13',\n",
       " '2000-01-01 00:01:38 fnard:11 fnok:10 cark:7 gnuck:47',\n",
       " '2000-01-01 00:01:43 fnard:2 fnok:2 cark:20 gnuck:35',\n",
       " '2000-01-01 00:01:53 fnard:12 fnok:11 cark:20 gnuck:3',\n",
       " '2000-01-01 00:01:54 fnard:6 fnok:6 cark:16 gnuck:18',\n",
       " '2000-01-01 00:02:01 fnard:9 fnok:10 cark:17 gnuck:15',\n",
       " '2000-01-01 00:02:08 fnard:0 fnok:6 cark:3 gnuck:49',\n",
       " '2000-01-01 00:02:10 fnard:15 fnok:12 cark:3 gnuck:14',\n",
       " '2000-01-01 00:02:17 fnard:7 fnok:-1 cark:18 gnuck:25',\n",
       " '2000-01-01 00:02:26 fnard:10 fnok:0 cark:4 gnuck:26',\n",
       " '2000-01-01 00:02:35 fnard:4 fnok:17 cark:14 gnuck:6',\n",
       " '2000-01-01 00:02:43 fnard:10 fnok:15 cark:19 gnuck:2',\n",
       " '2000-01-01 00:02:50 fnard:8 fnok:16 cark:7 gnuck:1',\n",
       " '2000-01-01 00:02:55 fnard:17 fnok:4 cark:17 gnuck:23',\n",
       " '2000-01-01 00:03:05 fnard:7 fnok:14 cark:18 gnuck:8',\n",
       " '2000-01-01 00:03:10 fnard:12 fnok:19 cark:7 gnuck:43',\n",
       " '2000-01-01 00:03:15 fnard:4 fnok:4 cark:4 gnuck:50',\n",
       " '2000-01-01 00:03:17 fnard:17 fnok:4 cark:13 gnuck:22',\n",
       " '2000-01-01 00:03:27 fnard:19 fnok:1 cark:18 gnuck:0',\n",
       " '2000-01-01 00:03:32 fnard:10 fnok:11 cark:4 gnuck:40',\n",
       " '2000-01-01 00:03:40 fnard:5 fnok:10 cark:6 gnuck:45',\n",
       " '2000-01-01 00:03:46 fnard:19 fnok:20 cark:7 gnuck:36',\n",
       " '2000-01-01 00:03:49 fnard:7 fnok:15 cark:17 gnuck:44',\n",
       " '2000-01-01 00:03:52 fnard:7 fnok:8 cark:17 gnuck:11',\n",
       " '2000-01-01 00:03:59 fnard:1 fnok:4 cark:11 gnuck:21',\n",
       " '2000-01-01 00:04:09 fnard:12 fnok:-1 cark:0 gnuck:48',\n",
       " '2000-01-01 00:04:10 fnard:4 fnok:19 cark:16 gnuck:30',\n",
       " '2000-01-01 00:04:14 fnard:18 fnok:7 cark:5 gnuck:11',\n",
       " '2000-01-01 00:04:18 fnard:19 fnok:11 cark:3 gnuck:29',\n",
       " '2000-01-01 00:04:21 fnard:18 fnok:14 cark:14 gnuck:35',\n",
       " '2000-01-01 00:04:23 fnard:0 fnok:19 cark:20 gnuck:17',\n",
       " '2000-01-01 00:04:30 fnard:7 fnok:7 cark:6 gnuck:42',\n",
       " '2000-01-01 00:04:34 fnard:14 fnok:2 cark:13 gnuck:39',\n",
       " '2000-01-01 00:04:41 fnard:19 fnok:14 cark:12 gnuck:32',\n",
       " '2000-01-01 00:04:51 fnard:19 fnok:18 cark:20 gnuck:34',\n",
       " '2000-01-01 00:04:57 fnard:3 fnok:1 cark:20 gnuck:16',\n",
       " '2000-01-01 00:05:04 fnard:9 fnok:2 cark:15 gnuck:19',\n",
       " '2000-01-01 00:05:12 fnard:7 fnok:2 cark:18 gnuck:36',\n",
       " '2000-01-01 00:05:16 fnard:12 fnok:14 cark:14 gnuck:47',\n",
       " '2000-01-01 00:05:22 fnard:18 fnok:14 cark:0 gnuck:47',\n",
       " '2000-01-01 00:05:28 fnard:11 fnok:9 cark:10 gnuck:23',\n",
       " '2000-01-01 00:05:33 fnard:19 fnok:16 cark:13 gnuck:41',\n",
       " '2000-01-01 00:05:38 fnard:6 fnok:20 cark:16 gnuck:18',\n",
       " '2000-01-01 00:05:45 fnard:4 fnok:4 cark:4 gnuck:16',\n",
       " '2000-01-01 00:05:51 fnard:8 fnok:5 cark:6 gnuck:19',\n",
       " '2000-01-01 00:05:53 fnard:10 fnok:14 cark:10 gnuck:26',\n",
       " '2000-01-01 00:05:55 fnard:10 fnok:0 cark:16 gnuck:9',\n",
       " '2000-01-01 00:06:03 fnard:10 fnok:3 cark:6 gnuck:1',\n",
       " '2000-01-01 00:06:05 fnard:5 fnok:4 cark:14 gnuck:12',\n",
       " '2000-01-01 00:06:14 fnard:17 fnok:12 cark:9 gnuck:45',\n",
       " '2000-01-01 00:06:17 fnard:9 fnok:7 cark:6 gnuck:46',\n",
       " '2000-01-01 00:06:25 fnard:6 fnok:15 cark:3 gnuck:44',\n",
       " '2000-01-01 00:06:27 fnard:3 fnok:10 cark:12 gnuck:29',\n",
       " '2000-01-01 00:06:28 fnard:9 fnok:9 cark:17 gnuck:7',\n",
       " '2000-01-01 00:06:31 fnard:16 fnok:19 cark:12 gnuck:48',\n",
       " '2000-01-01 00:06:33 fnard:2 fnok:2 cark:7 gnuck:40',\n",
       " '2000-01-01 00:06:37 fnard:10 fnok:5 cark:17 gnuck:32',\n",
       " '2000-01-01 00:06:47 fnard:-1 fnok:1 cark:17 gnuck:20',\n",
       " '2000-01-01 00:06:53 fnard:3 fnok:0 cark:8 gnuck:27',\n",
       " '2000-01-01 00:06:56 fnard:2 fnok:16 cark:8 gnuck:14',\n",
       " '2000-01-01 00:06:59 fnard:19 fnok:11 cark:17 gnuck:19',\n",
       " '2000-01-01 00:07:08 fnard:16 fnok:17 cark:7 gnuck:20',\n",
       " '2000-01-01 00:07:17 fnard:19 fnok:5 cark:9 gnuck:24',\n",
       " '2000-01-01 00:07:27 fnard:2 fnok:15 cark:18 gnuck:36',\n",
       " '2000-01-01 00:07:28 fnard:4 fnok:11 cark:12 gnuck:-1',\n",
       " '2000-01-01 00:07:35 fnard:9 fnok:8 cark:11 gnuck:1',\n",
       " '2000-01-01 00:07:36 fnard:3 fnok:19 cark:10 gnuck:49',\n",
       " '2000-01-01 00:07:38 fnard:20 fnok:19 cark:10 gnuck:22',\n",
       " '2000-01-01 00:07:41 fnard:6 fnok:14 cark:-1 gnuck:47',\n",
       " '2000-01-01 00:07:43 fnard:14 fnok:15 cark:19 gnuck:7',\n",
       " '2000-01-01 00:07:53 fnard:11 fnok:3 cark:4 gnuck:18',\n",
       " '2000-01-01 00:07:57 fnard:1 fnok:20 cark:11 gnuck:29',\n",
       " '2000-01-01 00:07:59 fnard:-1 fnok:20 cark:12 gnuck:20',\n",
       " '2000-01-01 00:08:00 fnard:16 fnok:19 cark:3 gnuck:42',\n",
       " '2000-01-01 00:08:07 fnard:15 fnok:2 cark:13 gnuck:30',\n",
       " '2000-01-01 00:08:15 fnard:4 fnok:17 cark:18 gnuck:11',\n",
       " '2000-01-01 00:08:17 fnard:1 fnok:9 cark:11 gnuck:29',\n",
       " '2000-01-01 00:08:19 fnard:6 fnok:-1 cark:5 gnuck:4',\n",
       " '2000-01-01 00:08:27 fnard:8 fnok:4 cark:6 gnuck:22',\n",
       " '2000-01-01 00:08:28 fnard:0 fnok:14 cark:1 gnuck:27',\n",
       " '2000-01-01 00:08:30 fnard:20 fnok:1 cark:7 gnuck:11',\n",
       " '2000-01-01 00:08:35 fnard:2 fnok:15 cark:6 gnuck:0',\n",
       " '2000-01-01 00:08:42 fnard:20 fnok:16 cark:15 gnuck:10',\n",
       " '2000-01-01 00:08:48 fnard:20 fnok:-1 cark:12 gnuck:42',\n",
       " '2000-01-01 00:08:57 fnard:12 fnok:13 cark:10 gnuck:29',\n",
       " '2000-01-01 00:08:59 fnard:13 fnok:0 cark:17 gnuck:12']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quazyilx.take(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to work with the RDD to make it into a more structure format. In the following cell, modify the code to create a **function or class** called `quazyilx_class` that processes a line and returns it as a dictionary-like structure, with attributes for the `.time`, `.fnard`, `.fnok` and `.cark`. \n",
    "\n",
    "You will need to define the Regular Expression and complete the class. The scaffolding has been provided for you. Use the helpful website [https://regex101.com/](https://regex101.com/) to build your regex to extract all the variables from the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os,datetime,re\n",
    "\n",
    "QUAZYILX_RE = \"(\\d{4}\\-\\d{2}\\-\\d{2}\\s\\d{2}\\:\\d{2}\\:\\d{2})\\s(fnard\\:-?\\d+)\\s(fnok\\:-?\\d+)\\s(cark\\:-?\\d+)\\s(gnuck\\:-?\\d+)\"\n",
    "quazyilx_re = re.compile(QUAZYILX_RE)\n",
    "\n",
    "class quazyilx_class():\n",
    "    import os,datetime,re\n",
    "    from pyspark.sql import Row\n",
    "    \n",
    "    def __init__(self,line):\n",
    "        \n",
    "        temp_m = quazyilx_re.search(line)\n",
    "        self.time = datetime.datetime.strptime(temp_m.group(1), \"%Y-%m-%d %H:%M:%S\")\n",
    "        self.fnard = temp_m.group(2).split(':')[-1]\n",
    "        self.fnok = temp_m.group(3).split(':')[-1]\n",
    "        self.cark = temp_m.group(4).split(':')[-1]\n",
    "        self.gnuck = temp_m.group(5).split(':')[-1]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will then need to turn the quazyilx RDD into a `Row()` object. This is somewhat similar to a dictionary format. This format means you can query different \"variables\" from your RDD at scale. You can make this structure with a lambda function, like this:\n",
    "\n",
    "```(python)\n",
    "lambda q: Row(datetime=q.datetime.isoformat(),fnard=q.fnard,fnok=q.fnok,cark=q.cark,gnuck=q.gnuck))\n",
    "```\n",
    "\n",
    "Alternatively, you can add a new method to the Quazyilx class called `.Row()` that returns a Row. All of these ways are more or less equivalent. You just need to pick one of them.  You may find it useful to look at [this documentation](http://spark.apache.org/docs/latest/sql-programming-guide.html#inferring-the-schema-using-reflection).\n",
    "\n",
    "In the next cell, create an RDD called `line` that converts the `quazyilx` RDD into a `Row()` object using the `quazyilx_class` and **cache** the result.\n",
    "\n",
    "**Remember, start with a smaller set of data before moving to the entire large dataset by using the `.sample()` method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#small_qua = quazyilx.sample(fraction=0.5, withReplacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = quazyilx.map(lambda q: Row(datetime = quazyilx_class(q).time.isoformat(),\n",
    "                                   fnard = quazyilx_class(q).fnard, \n",
    "                                   fnok = quazyilx_class(q).fnok,\n",
    "                                   cark = quazyilx_class(q).cark,\n",
    "                                   gnuck = quazyilx_class(q).gnuck))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.quazyilx_class at 0x7f4d090a9f10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quazyilx_class('2000-01-01 00:08:59 fnard:13 fnok:0 cark:17 gnuck:12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[3] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the first 10 rows to make sure everything is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(datetime='2000-01-01T00:00:03', fnard='7', fnok='8', cark='19', gnuck='25'),\n",
       " Row(datetime='2000-01-01T00:00:08', fnard='14', fnok='19', cark='16', gnuck='37'),\n",
       " Row(datetime='2000-01-01T00:00:17', fnard='12', fnok='11', cark='12', gnuck='8'),\n",
       " Row(datetime='2000-01-01T00:00:22', fnard='18', fnok='16', cark='3', gnuck='8'),\n",
       " Row(datetime='2000-01-01T00:00:32', fnard='7', fnok='16', cark='7', gnuck='37'),\n",
       " Row(datetime='2000-01-01T00:00:40', fnard='6', fnok='14', cark='3', gnuck='30'),\n",
       " Row(datetime='2000-01-01T00:00:47', fnard='11', fnok='10', cark='17', gnuck='7'),\n",
       " Row(datetime='2000-01-01T00:00:55', fnard='9', fnok='14', cark='13', gnuck='30'),\n",
       " Row(datetime='2000-01-01T00:00:56', fnard='10', fnok='1', cark='7', gnuck='6'),\n",
       " Row(datetime='2000-01-01T00:00:59', fnard='11', fnok='11', cark='12', gnuck='18')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will calculate the following using Spark RDD and save into dictionary objects as shown in the scaffolding code:\n",
    "\n",
    "1. The number of rows in the dataset\n",
    "1. The number of lines that have -1 for `fnard`, `fnok`, `cark` and `gnuck`.\n",
    "1. The number of lines that have -1 for `fnard` but have `fnok > 5` and `cark > 5`\n",
    "1. The first (earliest/smallest) datetime in the dataset\n",
    "1. The first (earliest/smallest) datetime that has -1 for all of the values\n",
    "1. The last (latest/largest) datetime in the dataset\n",
    "1. The last (latest/largest) datetime that has a -1 for all of the values\n",
    "\n",
    "Place each query into each of the following seven cells and run it to get the results. Remember, running the query statement itself will not give you the results you want. You need to do something else to \"get\" the result.\n",
    "\n",
    "**Note: in development testing, the first query may take approximately 10-15 minutes to run with the cluster configuration for this assignment (1 master, 4 task nodes of m5.xlarge). If you cache() correctly, all subsequent queries should take no more than 5 seconds.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all answers in this dictionary with keys 'q1','q2','q3',...\n",
    "dict_answers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dict_answers['q1'] = line.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = line.toDF(['datetime','fnard','fnok','cark','gnuck'])\n",
    "df_n1 = df[df['fnard'] == \"-1\"]\n",
    "df_n2 = df_n1[df_n1['fnok'] == \"-1\"]\n",
    "df_n3 = df_n2[df_n2['cark'] == \"-1\"]\n",
    "df_n4 = df_n3[df_n3['gnuck'] == \"-1\"].cache()\n",
    "dict_answers['q2'] = df_n4.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fn = df_n1[df_n1['fnok'] > 5]\n",
    "final = fn[fn['cark']> 5]\n",
    "dict_answers['q3'] = final.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"serve RDD 141 with partitions 0\" java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.net.PlainSocketImpl.socketAccept(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\n",
      "\tat java.net.ServerSocket.implAccept(ServerSocket.java:560)\n",
      "\tat java.net.ServerSocket.accept(ServerSocket.java:528)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:58)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sorted_date = line.cache().sortBy(lambda x: x[0])\n",
    "dict_answers['q4'] = sorted_date.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_date_bad = df_n4.sort(['datetime'],ascending=True)                          \n",
    "dict_answers['q5'] = sorted_date_bad.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dict_answers['q6'] = sorted_date.top(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_date_bad_1 = df_n4.sort(['datetime'],ascending=False)   \n",
    "dict_answers['q7'] = sorted_date_bad_1.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Run the following cell to export your final dictionary results into a json file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dump(str(dict_answers), fp = open('problem-2-soln.json','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you finish this problem, click on the File -> 'Save and Checkpoint' in the menu bar to make sure that the latest version of the workbook file is saved. Also, before you close this notebook and move on, make sure you disconnect your SparkContext, otherwise you will not be able to re-allocate resources. Remember, you will commit the .ipynb file to the repository for submission (in the master node terminal.)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
